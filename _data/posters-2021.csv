ID,Created,Last Modified,Title,Abstract,Primary Contact Author Name,Primary Contact Author Email,Authors,Author Names,Author Emails,Track Name,Primary Subject Area,Secondary Subject Areas,Conflicts,Domains,Assigned,Completed,% Completed,Bids,Discussion,Status,Requested For Author Feedback,Author Feedback Submitted?,Requested For Camera Ready,Camera Ready Submitted?,Requested For Presentation,Files,Number of Files,Supplementary Files,Number of Supplementary Files,Reviewers,Reviewer Emails,MetaReviewers,MetaReviewer Emails,SeniorMetaReviewers,SeniorMetaReviewerEmails,Q1 (Submission Type),Type,Poster,Floor,Poster Presenter,Talk Day,Talk Presenter
1,3/12/2021 11:44:54 AM -08:00,7/2/2021 1:02:11 PM -07:00,Complex Coordinate-Based Meta-Analysis with Probabilistic Programming,"With the growing number of published functional magnetic resonance imaging (fMRI) studies, meta-analysis databases and models have become an integral part of brain mapping research. Coordinate-based meta-analysis (CBMA) databases are built by extracting both coordinates of reported peak activations and term associations using natural language processing techniques from neuroimaging studies. Solving term-based queries on these databases makes it possible to obtain statistical maps of the brain related to specific cognitive processes. However, existing tools for analysing CBMA data are limited in their expressivity to propositional logic, restricting the variety of their queries. Moreover, with tools like Neurosynth, term-based queries on multiple terms often lead to power failure, because too few studies from the database contribute to the statistical estimations. We design a probabilistic domain-specific language (DSL) standing on Datalog and one of its probabilistic extensions, CP-Logic, for expressing and solving complex logic-based queries. We show how CBMA databases can be encoded as probabilistic programs. Using the joint distribution of their Bayesian network translation, we show that solutions of queries on these programs compute the right probability distributions of voxel activations. We explain how recent lifted query processing algorithms make it possible to scale to the size of large neuroimaging data, where knowledge compilation techniques fail to solve queries fast enough for practical applications. Finally, we introduce a method for relating studies to terms probabilistically, leading to better solutions for two-term conjunctive queries (CQs) on smaller databases. We demonstrate results for two-term CQs, both on simulated meta-analysis databases and on the widely used Neurosynth database.",Valentin Iovene,valentin@too.gy,Valentin Iovene (Inria)*; Gaston E Zanitti (Inria); Demian Wassermann (Inria),"Iovene, Valentin*; Zanitti, Gaston E; Wassermann, Demian",valentin@too.gy*; gaston.zanitti@inria.fr; demian.wassermann@inria.fr,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,0,inria.fr;cea.fr,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"paper.pdf (341,196 bytes)",1,,0,,,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Iovene, Valentin*",,
2,4/16/2021 8:26:13 AM -07:00,4/16/2021 8:26:13 AM -07:00,Transforming Worlds: Automated Involutive MCMC for Open-Universe Probabilistic Models,"Open-universe probabilistic models enable Bayesian inference about how many objects underlie data, and how they are related. Effective inference in OUPMs remains a challenge, however, often requiring the use of custom, trans-dimensional MCMC kernels, based on heuristics, deep learning, or domain knowledge, that can be difficult to derive and to implement correctly. This paper adapts the recently introduced involutive MCMC framework to the open-universe setting, and shows how error-prone aspects of kernel design and implementation (e.g., the computation of valid accept/reject probabilities) can be automated, using techniques from probabilistic and differentiable programming. The result is an intuitive design space for MCMC kernels for OUPMs: users write programs that propose incremental changes to possible worlds, creating, deleting, or modifying objects according to arbitrary application-specific logic, and their proposals are automatically converted into stationary MCMC kernels. We demonstrate in preliminary experiments that data-driven involutive MCMC kernels outperform generic probabilistic programming language inference, as well as generic birth/death reversible-jump kernels without application-specific logic.",George PB Matheos,georgematheos@berkeley.edu,George PB Matheos (MIT)*; Alexander K. Lew (MIT); Matin Ghavamizadeh (MIT); Stuart Russell (UC Berkeley); Marco Cusumano-Towner (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Matheos, George PB*; Lew, Alexander K.; Ghavamizadeh, Matin; Russell, Stuart; Cusumano-Towner, Marco; Mansinghka, Vikash",georgematheos@berkeley.edu*; alexlew@mit.edu; mghavami@berkeley.edu; russell@berkeley.edu; marcoct@mit.edu; vkm@mit.edu,PROBPROG2021,"Languages, Tools, and Systems",Artificial and Natural Intelligence; The Practice of Probabilistic Programming,7,mit.edu; berkeley.edu,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"transforming_worlds_automated_involutive_mcmc_for_open_universe_probabilistic_models-2.pdf (9,042,275 bytes)",1,,0,,,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Syndicated Submission,Syndicated Submission,Wed,,George PB Matheos (MIT)*,,
3,4/20/2021 6:39:55 AM -07:00,4/20/2021 6:42:57 AM -07:00,Automated Termination Analysis of Polynomial Probabilistic Programs,"The termination behavior of probabilistic programs depends on the outcomes of random assignments. Almost sure termination (AST) is concerned with the question whether a program terminates with probability one on all possible inputs. Positive almost sure termination (PAST) focuses on termination in a finite expected number of steps. This paper presents a fully automated approach to the termination analysis of probabilistic while-programs whose guards and expressions are polynomial expressions. As proving (positive) AST is undecidable in general, existing proof rules typically provide sufficient conditions. These conditions mostly involve constraints on supermartingales. We consider four proof rules from the literature and extend these with generalizations of existing proof rules for (P)AST. We automate the resulting set of proof rules by effectively computing asymptotic bounds on polynomials over the program variables. These bounds are used to decide the sufficient conditions – including the constraints on supermartingales – of a proof rule. Our software tool Amber can thus check AST, PAST, as well as their negations for a large class of polynomial probabilistic programs, while carrying out the termination reasoning fully with polynomial witnesses. Experimental results show the merits of our generalized proof rules and demonstrate that Amber can handle probabilistic programs that are out of reach for other state-of-the-art tools.",Marcel Moosbrugger,marcel.moosbrugger@tuwien.ac.at,Marcel Moosbrugger (TU Wien)*; Ezio Bartocci (TU Wien); Joost-Pieter Katoen (RWTH Aachen University); Laura Kovaćs (TU Wien),"Moosbrugger, Marcel*; Bartocci, Ezio; Katoen, Joost-Pieter; Kovaćs, Laura",marcel.moosbrugger@tuwien.ac.at*; ezio.bartocci@tuwien.ac.at; katoen@cs.rwth-aachen.de; laura.kovacs@tuwien.ac.at,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,0,tuwien.ac.at; rwth-aachen.de,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"paper.pdf (482,344 bytes)",1,,0,,,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Moosbrugger, Marcel*",,
4,4/30/2021 6:01:02 AM -07:00,4/30/2021 6:01:46 AM -07:00,Probabilistic inductive constraint logic,"Probabilistic logical models deal effectively with uncertain relations and entities typical of many real world domains. In the field of probabilistic logic programming usually the aim is to learn these kinds of models to predict specific atoms or predicates of the domain, called target atoms/predicates. However, it might also be useful to learn classifiers for interpreta- tions as a whole: to this end, we consider the models produced by the inductive constraint logic system, represented by sets of integrity constraints, and we propose a probabilistic version of them. Each integrity constraint is annotated with a probability, and the resulting probabilistic logical constraint model assigns a probability of being positive to interpreta- tions. To learn both the structure and the parameters of such probabilistic models we pro- pose the system PASCAL for “probabilistic inductive constraint logic”. Parameter learning can be performed using gradient descent or L-BFGS. PASCAL has been tested on 11 data- sets and compared with a few statistical relational systems and a system that builds rela- tional decision trees (TILDE): we demonstrate that this system achieves better or compara- ble results in terms of area under the precision–recall and receiver operating characteristic curves, in a comparable execution time.",Elena Bellodi,elena.bellodi@unife.it,Fabrizio Riguzzi (Universita di Ferrara); Elena Bellodi (University of Ferrara)*; Riccardo Zese (University of Ferrara); Marco Alberti (University of Ferrara); Evelina Lamma (University of Ferrara),"Riguzzi, Fabrizio; Bellodi, Elena*; Zese, Riccardo; Alberti, Marco; Lamma, Evelina",fabrizio.riguzzi@unife.it; elena.bellodi@unife.it*; riccardo.zese@unife.it; marco.alberti@unife.it; evelina.lamma@unife.it,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,1,unife.it; unibo.it,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"2020_ProbabilisticInductiveConstraintLogic.pdf (1,885,227 bytes)",1,,0,,,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Riguzzi, Fabrizio; Bellodi, Elena*",,
5,4/30/2021 10:49:11 PM -07:00,4/30/2021 10:49:11 PM -07:00,SPPL: Probabilistic Programming with Fast Exact Symbolic Inference,"We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic programming language that automatically delivers exact solutions to a broad range of probabilistic inference queries. SPPL translates probabilistic programs into sum-product expressions, a new symbolic representation and associated semantic domain that extends standard sum-product networks to support mixed-type distributions, numeric transformations, logical formulas, and pointwise and set-valued constraints. We formalize SPPL via a novel translation strategy from probabilistic programs to sum-product expressions and give sound exact algorithms for conditioning on and computing probabilities of events. SPPL imposes a collection of restrictions on probabilistic programs to ensure they can be translated into sum-product expressions, which allow the system to leverage new techniques for improving the scalability of translation and inference by automatically exploiting probabilistic structure. We implement a prototype of SPPL with a modular architecture and evaluate it on benchmarks the system targets, showing that it obtains up to 3500x speedups over state-of-the-art symbolic systems on tasks such as verifying the fairness of decision tree classifiers, smoothing hidden Markov models, conditioning transformed random variables, and computing rare event probabilities.",Feras Saad,fsaad@mit.edu,Feras Saad (Massachusetts Institute of Technology)*; Martin Rinard (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Saad, Feras*; Rinard, Martin; Mansinghka, Vikash",fsaad@mit.edu*; rinard@csail.mit.edu; vkm@mit.edu,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,8,mit.edu,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,Yes,"2010.03485.pdf (2,234,326 bytes)",1,,0,,,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Syndicated Submission,Syndicated Submission,Wed,,"Saad, Feras*",Wed,"Saad, Feras*"
6,5/3/2021 1:39:05 PM -07:00,5/10/2021 4:26:03 AM -07:00,GPflux: A Library for Deep Gaussian Processes,"We introduce GPflux, a Python library for Bayesian deep learning with a strong emphasis on deep Gaussian processes (DGPs). Implementing DGPs is a challenging endeavour due to the various mathematical subtleties that arise when dealing with multivariate Gaussian distributions and the complex bookkeeping of indices. To date, there are no actively maintained, open-sourced and extendable libraries available that support research activities in this area. GPflux aims to fill this gap by providing a library with state-of-the-art DGP algorithms, as well as building blocks for implementing novel Bayesian and GP-based hierarchical models and inference schemes. GPflux is compatible with and built on top of the Keras deep learning eco-system. This enables practitioners to leverage tools from the deep learning community for building and training customised Bayesian models, and create hierarchical models that consist of Bayesian and standard neural network layers in a single coherent framework. GPflux relies on GPflow for many of its GP objects and operations, which makes it an efficient, modular and extensible library, while having a lean codebase.",Vincent Dutordoir,vd309@cam.ac.uk,Vincent Dutordoir (University of Cambridge)*; Eric L L Hambro (Facebook AI Research); Hugh Salimbeni (G-Research); John McLeod (Secondmind.ai); Felix Leibfried (PROWLER.io); Artem Artemev (Imperial College London); Mark van der Wilk (Imperial College London); James Hensman (PROWLER.io); Marc Deisenroth (University College London); ST John (Secondmind.ai),"Dutordoir, Vincent*; Hambro, Eric L L; Salimbeni, Hugh; McLeod, John; Leibfried, Felix; Artemev, Artem; van der Wilk, Mark; Hensman, James; Deisenroth, Marc; John, ST",vd309@cam.ac.uk*; eric.hambro@gmail.com; hugh.salimbeni@gmail.com; johnangusmcleod@gmail.com; felix.leibfried@gmail.com; a.artemev20@imperial.ac.uk; m.vdwilk@imperial.ac.uk; hensman@amazon.com; m.deisenroth@ucl.ac.uk; stjohnphd@gmail.com,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,3,cam.ac.uk;imperial.ac.uk;ucl.ac.uk;amazon.com;secondmind.ai,3,3,100%,0,Enabled (4),Accept (Abstract),No,No,No,No,Yes,"ProbProg_GPflux.pdf (453,205 bytes)",1,,0,Feras Saad (Massachusetts Institute of Technology); Tom Rainforth (University of Oxford); Xin Zhang (),fsaad@mit.edu; rainforth@stats.ox.ac.uk; xzhang@csail.mit.edu,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Thu,,"Dutordoir, Vincent*",Thu,"Dutordoir, Vincent*"
7,5/4/2021 3:19:39 PM -07:00,5/11/2021 3:25:19 PM -07:00,Inference in Network-based Epidemiological Simulations with Probabilistic Programming,"Accurate epidemiological models require parameter estimates that account for mobility patterns and social network structure. This work applies probabilistic programming to infer parameters in agent-based models. We represent mobility networks as degree-corrected stochastic block models and estimate their parameters from cell-phone co-location data. We use these networks in probabilistic programs to simulate the evolution of an epidemic, and condition on reported cases to infer disease transmission parameters. Our experiments demonstrate that the resulting models improve the accuracy-of-fit in multiple geographies relative to baselines that do not model network topology.",Niklas Smedemark-Margulies,smedemark-margulie.n@northeastern.edu,Niklas Smedemark-Margulies (Northeastern University)*; Robin Walters (Northeastern University); Heiko Zimmermann (Northeastern University); Lucas Laird (MIT Lincoln Laboratory); Neela Kaushik (MIT Lincoln Laboratory); Rajmonda S. Caceres (MIT Lincoln Laboratory); Jan-Willem van de Meent (Northeastern University),"Smedemark-Margulies, Niklas*; Walters, Robin; Zimmermann, Heiko; Laird, Lucas; Kaushik, Neela; Caceres, Rajmonda S.; van de Meent, Jan-Willem",smedemark-margulie.n@northeastern.edu*; r.walters@northeastern.edu; zimmermann.h@northeastern.edu; Lucas.Laird@ll.mit.edu; Neela.Kaushik@ll.mit.edu; Rajmonda.Caceres@ll.mit.edu; j.vandemeent@northeastern.edu,PROBPROG2021,The Practice of Probabilistic Programming,Statistics and Data Analysis,15,northeastern.edu; ll.mit.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"inference-in-network-based-epidemiological-simulations-with-probabilistic-programming.pdf (2,469,615 bytes)",1,,0,Emily Fertig (Google); Fritz Obermeyer (); Jan Kudlicka (Uppsala University),emilyaf@google.com; fobermey@broadinstitute.org; jan.kudlicka@it.uu.se,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Smedemark-Margulies, Niklas*",,
8,5/5/2021 12:08:35 PM -07:00,5/6/2021 9:15:23 AM -07:00,How to train your program ,"We present a Bayesian approach to  machine learning with probabilistic programs. In our approach, training on available data is implemented as inference on a hierarchical model. The posterior distribution of model parameters is then used to stochastically condition a complementary model, such that inference on new data yields the same posterior distribution of latent parameters corresponding to the new data as inference on a hierachical model on the combination of both previously available and new data, at a lower computation cost. We frame the approach as a design pattern of probabilistic programming referred to herein as `stump and fungus', and illustrate realization of the pattern on a didactic case study.",David Tolpin,david.tolpin@gmail.com,David Tolpin (Ben Gurion University of the Negev & PUB+)*,"Tolpin, David*",david.tolpin@gmail.com*,PROBPROG2021,Statistics and Data Analysis,The Practice of Probabilistic Programming,1,bgu.ac.il,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"paper.pdf (1,079,595 bytes)",1,,0,Avi Bryant (Gradient Retreat); Daniel Ritchie (Brown University); Emily Fertig (Google),avi@avibryant.com; daniel_ritchie@brown.edu; emilyaf@google.com,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Thu,,"Tolpin, David*",,
9,5/5/2021 2:57:51 PM -07:00,5/5/2021 2:57:51 PM -07:00,"Lambdas, tuples, ragged arrays, and complex numbers in Stan","We are introducing four new language features for Stan:  complex numbers, ragged arrays, tuples, and lambdas. There will be  basic types for complex scalars, vectors, and matrices, backed by arithmetic and special-function support in the Stan math library.  Ragged arrays will be homogeneous containers like Stan's existing  arrays.  Tuples provide a heterogeneous container type; adding names  gives us structs.  Simple function types enable us to deal with type  inference for functions.  We will provide lambdas with implicit  binding by value, which can be directly implemented via closures.",Bob Carpenter,bcarpenter@flatironinstitute.org,Bob Carpenter ()*,"Carpenter, Bob*",bcarpenter@flatironinstitute.org*,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,2,flatironinstitute.org; columbia.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"carpenter-probprog2021.pdf (489,829 bytes)",1,,0,Andy Gordon (Microsoft Research); Joseph Tassarotti (Boston College); Maria Gorinova (University of Edinburgh),adg@microsoft.com; tassarot@bc.edu; m.gorinova@ed.ac.uk,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Wed,,"Carpenter, Bob*",,
10,5/5/2021 11:16:40 PM -07:00,5/10/2021 5:56:12 PM -07:00,Few-shot Bayesian inference of 3D objects and scenes via probabilistic programs,"The human visual system can learn representations of novel 3D objects in real-time from just a handful of views. This learning reflects uncertainty and yields representations that can be used to robustly recognize objects in novel scenes, despite large variations in viewpoint, occlusion, lighting, resolution, and clutter. In contrast, deep learning approaches often requires expensive, offline training on large, labeled datasets, and yet still lack robustness, flexibility, and generalizability. In this work, we present a probabilistic, generative approach to 3D object model learning based on Bayesian inverse graphics and probabilistic programming. Our architecture variationally learns generative programs modeling 3D object shape and incorporates these programs into 3D generative scene graph models. Then, given a depth image of a scene, we use a Sequential Monte Carlo algorithm to infer posterior distributions over object poses and contact relationships.",Nishad Gothoskar,nishadg@mit.edu,Nishad Gothoskar (MIT)*; Ben Zinberg (MIT); Marco Cusumano-Towner (MIT); Falk Pollok (IBM Research AI); Joshua Tenenbaum (MIT); Dan Gutfreund (IBM); Vikash Mansinghka (Massachusetts Institute of Technology),"Gothoskar, Nishad*; Zinberg, Ben; Cusumano-Towner, Marco; Pollok, Falk; Tenenbaum, Joshua; Gutfreund, Dan; Mansinghka, Vikash",nishadg@mit.edu*; bzinberg@mit.edu; marcoct@mit.edu; Falk.Pollok@ibm.com; jbt@mit.edu; dgutfre@us.ibm.com; vkm@mit.edu,PROBPROG2021,Artificial and Natural Intelligence,,7,mit.edu; cmu.edu,3,3,100%,0,Enabled (0),Accept (Abstract),No,No,No,No,No,"ProbProg_3D_Vision_final.pdf (3,413,505 bytes)",1,,0,Hao Wu (); Jonathan Chen (Uber AI Labs); Robert Zinkov (University of Oxford),haowu@ccis.neu.edu; jonathanp.chen@gmail.com; zinkov@robots.ox.ac.uk,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Wed,,"Gothoskar, Nishad*",,
11,5/6/2021 12:28:12 AM -07:00,5/6/2021 12:30:16 AM -07:00,PPCheck: Verifying the Equivalence of Probabilistic Programs,"The surge of interest in probabilistic programming systems has propelled the necessity of testing and verification techniques of probabilistic programming to the front. Motivated by the presence of approximate inference techniques, in this paper we study approximate equivalence testing, a relaxation of the fundamental problem of equivalence testing. We propose the first efficient technique, called PPCheck, that accepts two programs if their corresponding distributions are within eps-distance and rejects if their distributions are eta-far. Our technique relies on the novel usage of conditioning and we provide a rigorous theoretical analysis of the soundness and completeness of our technique. We augment our theoretical contribution with a detailed empirical evaluation along with the contribution of a preliminary benchmark suite. Our evaluation demonstrates that PPCheck can handle probabilistic programs defined over support sizes > 2^14, and unlike the baseline approach, can correctly distinguish between close and far programs. We envision PPCheck will be used as a basic building block for verification and testing methodologies for probabilistic programming. ",Alexandru Dinu,alex.dinu07@gmail.com,Alexandru Dinu (National University of Singapore)*; Sourav Chakraborty (Indian Statistical Institute); Kuldeep S Meel (National University of Singapore),"Dinu, Alexandru*; Chakraborty, Sourav; Meel, Kuldeep S",alex.dinu07@gmail.com*; sourav@cmi.ac.in; meel@comp.nus.edu.sg,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,0,u.nus.edu,4,4,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"PROBPROG_2021_extended_abstract-merged.pdf (1,205,430 bytes)",1,,0,Adam Scibior (University of British Columbia); Angelika Kimmig (); Christine Tasson (); Steven Holtzen (Northeastern University),ascibior@cs.ubc.ca; angelika.kimmig@kuleuven.be; christine.tasson@irif.fr; s.holtzen@northeastern.edu,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Wed,,"Dinu, Alexandru*",,
12,5/6/2021 6:10:32 AM -07:00,5/11/2021 2:55:13 PM -07:00,Assessing Inference Quality for Probabilistic Programs using Multivariate Simulation Based Calibration,"The task of choosing an inference procedure in probabilistic programming is a subtle and complex problem, due to both the vast space of options and the difficulty of assessing inference quality. As such, it often no longer suffices to use standard heuristics for choosing certain classes of inference procedures that are expected to perform well on a given classes of models. Diagnostic tools that are specific to the choice of inference algorithm, such as MCMC convergence diagnostics, may also not be suitable for many of these algorithms. Hence it is crucial to have a principled approach that can compare arbitrary inference procedures that are specified as code. As one such approach, this paper presents Multivariate Simulation Based Calibration (mSBC), a multivariate extension to SBC  [Talts et al.2018], which assesses inference quality using the approximate posterior averaged across data simulated from the prior. This paper addresses how to run mSBC on models with a mixture of continuous and discrete parameters; open-universe models where the number of variables is uncertain; and models where existence of one or more parameters in a given trace is uncertain.",Sharan Yalburgi,sharanyalburgi@gmail.com,"Sharan Yalburgi (BITS Pilani)*; Cameron Freer (Massachusetts Institute of Technology); Jameson Quinn (MIT); Veronica Weiner (MIT); Sam A Witty (University of Massachusetts, Amherst); Vikash Mansinghka (Massachusetts Institute of Technology)","Yalburgi, Sharan*; Freer, Cameron; Quinn, Jameson; Weiner, Veronica; Witty, Sam A; Mansinghka, Vikash",sharanyalburgi@gmail.com*; freer@mit.edu; drquinn@mit.edu; vsw@mit.edu; switty@cs.umass.edu; vkm@mit.edu,PROBPROG2021,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",9,mit.edu; cs.umass.edu; goa.bits-pilani.ac.in; cam.ac.uk,4,4,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"Multivariate_SBC-3.pdf (1,207,529 bytes)",1,,0,Bob Carpenter (); Ekansh Sharma (University of Toronto); Martin Jankowiak (); Nimar Arora (Bayesian Logic),bcarpenter@flatironinstitute.org; ekansh@cs.toronto.edu; mjankowi@broadinstitute.org; nimar.arora@gmail.com,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Yalburgi, Sharan*",,
13,5/6/2021 6:40:07 AM -07:00,5/6/2021 6:40:07 AM -07:00,From Probabilistic NetKAT to ProbLog: New Algorithms for Inference and Learning in Probabilistic Networks,"We consider the problem of modelling probabilistic networks, where one is interested in modelling packets moving through an unreliable network, with each link carrying a certain probability of failure. The domain-specific language Probabilistic NetKAT provides a clear syntax and semantics for specifying such networks in a filter-based (rather than state-based) manner. We introduce a formal translation to transform this domain-specific language into ProbLog, a popular probabilistic logic programming language. We show how employing this translation allows one to take advantage of different inference and learning procedures designed for ProbLog in the context of Probabilistic NetKAT. This illustrates how transforming domain-specific languages to general-purpose probabilistic programming languages can provide a kind of rapid prototyping.",Timothy van Bremen,timothy.vanbremen@cs.kuleuven.be,Birthe Van den Berg (KU Leuven); Timothy van Bremen (KU Leuven)*; Vincent Derkinderen (KU Leuven); Angelika Kimmig (KU Leuven); Tom Schrijvers (KU Leuven); Luc De Raedt (KU Leuven),"Van den Berg, Birthe; van Bremen, Timothy*; Derkinderen, Vincent; Kimmig, Angelika; Schrijvers, Tom; De Raedt, Luc",birthe.vandenberg@cs.kuleuven.be; timothy.vanbremen@cs.kuleuven.be*; vincent.derkinderen@kuleuven.be; angelika.kimmig@cs.kuleuven.be; tom.schrijvers@kuleuven.be; luc.deraedt@cs.kuleuven.be,PROBPROG2021,"Languages, Tools, and Systems",,7,kuleuven.be,2,2,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"From Probabilistic NetKAT to ProbLog.pdf (471,962 bytes)",1,,0,Alexander Lew (MIT); Ulrich Schaechtle (MIT),alexlew@mit.edu; ulli@mit.edu,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Thu,,"Van den Berg, Birthe",,
14,5/6/2021 8:15:35 AM -07:00,5/10/2021 11:21:00 AM -07:00,Automatic Guide Generation for Stan via NumPyro,"Stan is a very popular probabilistic language with a state-of-the-art HMC sampler but it only offers a limited choice of algorithms for black-box variational inference.
In this paper, we show that using our recently proposed compiler from Stan to Pyro, Stan users can easily try the set of algorithms implemented in Pyro for black-box variational inference.
We evaluate our approach on PosteriorDB, a database of Stan models with corresponding data and reference posterior samples.
Results show that the eight algorithms available in Pyro offer a range of possible compromises between complexity and accuracy.
This paper illustrates that compiling Stan to another probabilistic language can be used to leverage new features for Stan users, and give access to a large set of examples for language developers who implement these new features.",Guillaume Baudart,guillaume.baudart@inria.fr,"Guillaume Baudart (Inria Paris, École normale supérieure - PSL University)*; Louis Mandel (IBM Research)","Baudart, Guillaume*; Mandel, Louis",guillaume.baudart@inria.fr*; lmandel@us.ibm.com,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,8,inria.fr; ibm.com,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,Yes,"probprog21-deepstan.pdf (526,501 bytes)",1,,0,Andy Gordon (Microsoft Research); Joseph Tassarotti (Boston College); Maria Gorinova (University of Edinburgh),adg@microsoft.com; tassarot@bc.edu; m.gorinova@ed.ac.uk,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Thu,,"Baudart, Guillaume*",Wed,"Baudart, Guillaume*"
15,5/6/2021 8:41:20 AM -07:00,5/6/2021 8:41:20 AM -07:00,High-Dimensional Bayesian Workflow in Pyro,"I describe a workflow for incrementally building pipelines to analyzing high-dimensional data in Pyro.  This workflow has evolved over a few years of practical modeling at Uber and the Broad Institute.  While the individual components of these pipeline are well-known, workflow efficiency demands that code changes to upstream components do not break previous coding effort on downstream components.  Pyro's approaches to this challenge may be of interest to PPL system developers.",Fritz H Obermeyer,fritz.obermeyer@gmail.com,Fritz H Obermeyer (Broad Institute)*,"Obermeyer, Fritz H*",fritz.obermeyer@gmail.com*,PROBPROG2021,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",2,broadinstitute.org,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"main.pdf (271,103 bytes)",1,,0,Bradley Gram-Hansen (University of Oxford); Chad Scherrer (RelationalAI); Christian Weilbach (University of British Columbia),bradley@intelligentnetworks.ai; chad.scherrer@gmail.com; ch_weil@topiq.es,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Obermeyer, Fritz H*",,
16,5/6/2021 10:39:18 AM -07:00,5/11/2021 12:06:42 PM -07:00,Statically Bounded-Memory Delayed Sampling for Probabilistic Streams,"Probabilistic programming languages aid developers performing Bayesian inference. These languages provide programming constructs and tools for probabilistic modeling and automating the process of developing a probabilistic inference procedure. Prior work introduced a probabilistic programming language, ProbZelus, to extend probabilistic programming functionality to unbounded streams of data. A key innovation was to demonstrate that the delayed sampling inference algorithm could be extended to work in a streaming context.
ProbZelus showed that while delayed sampling could be effectively deployed on some programs, depending on the probabilistic model under consideration, delayed sampling is not guaranteed to use a bounded amount of memory over the course of the execution of the program.

In this extended abstract, we the present conditions on a probabilistic program's execution under which delayed sampling will execute in bounded memory.
The two conditions are dataflow properties of the core operations of delayed sampling: the m-consumed property and the unseparated path property.
A program executes in bounded memory under delayed sampling if, and only if, it satisfies the m-consumed and unseparated path properties.
We propose a static analysis that abstracts over these properties to soundly ensure that any program that passes the analysis satisfies these properties, and thus executes in bounded memory under delayed sampling.
",Louis Mandel,lmandel@us.ibm.com,"Eric Atkinson (MIT); Guillaume Baudart (Inria Paris, École normale supérieure - PSL University); Louis Mandel (IBM Research)*; Charles Yuan (MIT); Michael Carbin (MIT)","Atkinson, Eric; Baudart, Guillaume; Mandel, Louis*; Yuan, Charles; Carbin, Michael",eatkinson@csail.mit.edu; guillaume.baudart@inria.fr; lmandel@us.ibm.com*; chenhuiy@csail.mit.edu; mcarbin@csail.mit.edu,PROBPROG2021,"Languages, Tools, and Systems",,11,ibm.com;mit.edu;inria.fr;ens.fr;lri.fr,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"probprog21_probzelus-analysis.pdf (411,537 bytes)",1,,0,Daniel Lundén (KTH Royal Institute of Technology); Ekansh Sharma (University of Toronto); Johannes Borgström (),dlunde@kth.se; ekansh@cs.toronto.edu; johannes.borgstrom@it.uu.se,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Mandel, Louis*",,
17,5/6/2021 3:23:15 PM -07:00,5/9/2021 8:30:06 AM -07:00,Gaussian Processes to speed up MCMC with automatic exploratory-exploitation effect,"We derive a  two-stage Metropolis-Hastings algorithm for sampling probabilistic models, whose log-likelihood is computationally expensive to evaluate, by using a surrogate Gaussian Process (GP) model. The key feature of the approach, and the difference w.r.t. previous works, is the ability to learn the target distribution from scratch (while sampling), and so without the need of pre-computing the GP. We have obtained this feature by marginalising out the GP distributed function, which makes the acceptance ratio explicitly dependent on the variance of the GP. This approach is then extended to Metropolis-Adjusted Langevin algorithm (MALA). Numerical experiments demonstrates the effectiveness of the method, which can automatically adapt to the complexity of the target distribution. ",Alessio Benavoli,alessio.benavoli@tcd.ie,Alessio Benavoli (Trinity College Dublin)*; Jason Wyse (Trinity College Dublin); Arthur White (Trinity College Dublin),"Benavoli, Alessio*; Wyse, Jason; White, Arthur",alessio.benavoli@tcd.ie*; wyseja@tcd.ie; arwhite@tcd.ie,PROBPROG2021,The Practice of Probabilistic Programming,Statistics and Data Analysis,0,idsia.ch,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"GP_MCMC(2).pdf (634,419 bytes)",1,,0,Theofanis Karaletsos (Facebook); Tom Rainforth (University of Oxford); Ulrich Schaechtle (MIT),theofanis.karaletsos@gmail.com; rainforth@stats.ox.ac.uk; ulli@mit.edu,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Thu,,"Benavoli, Alessio*",,
18,5/6/2021 3:25:24 PM -07:00,5/11/2021 5:31:23 PM -07:00,Composing Importance Samplers with Lenses,"We define a Cartesian lens category for properly weighted probabilistic program evaluation. This structure provides semantics for composing samplers in sequence and in parallel, and provides a basis for propagating importance weights both forwards and backwards to perform filtering and smoothing. Forward filtering recovers existing importance sampling methods for probabilistic programs, whereas backward updates can be used to implement localized proposals that leave the smoothing distribution invariant. We describe a concrete algorithm for forward filtering with backwards simulation, and sketch how the backwards sweep can be used to implement other operations, such as localized (approximate) Gibbs updates. ",Eli Z Sennesh,esennesh@ccis.neu.edu,Eli Z Sennesh (Northeastern University)*; Sam Stites (Northeastern University); Jan-Willem van de Meent (Northeastern University),"Sennesh, Eli Z*; Stites, Sam; van de Meent, Jan-Willem",esennesh@ccis.neu.edu*; stites.s@northeastern.edu; j.vandemeent@northeastern.edu,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,8,northeastern.edu; neu.edu,4,3,75%,0,Enabled (0),Accept (Abstract),No,No,No,No,No,"probprog_2021_lensy_combinators.pdf (569,574 bytes)",1,,0,Cameron Freer (Massachusetts Institute of Technology); Feras Saad (Massachusetts Institute of Technology); Hugo Paquet (University of Oxford); Ohad Kammar (University of Oxford),freer@mit.edu; fsaad@mit.edu; hugo.paquet@cs.ox.ac.uk; ohad.kammar@ed.ac.uk,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Wed,,"Sennesh, Eli Z*",,
19,5/6/2021 4:01:33 PM -07:00,5/11/2021 5:17:19 PM -07:00,Nested Variational Inference,"We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and additionally provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to learn samplers targeting (a) an unnormalized density using annealing and (b) the posterior of a hidden Markov model. We observe improved sample quality in terms of log expected weight and effective sample size.",Heiko Zimmermann,zimmermann.h@northeastern.edu,Heiko Zimmermann (Northeastern University)*; Hao Wu (Northeastern University); Babak Esmaeili (Northeastern University); Sam Stites (Northeastern University); Jan-Willem van de Meent (Northeastern University),"Zimmermann, Heiko*; Wu, Hao; Esmaeili, Babak; Stites, Sam; van de Meent, Jan-Willem",zimmermann.h@northeastern.edu*; wu.hao10@northeastern.edu; esmaeili.b@northeastern.edu; stites.s@northeastern.edu; j.vandemeent@northeastern.edu,PROBPROG2021,Statistics and Data Analysis,,8,northeastern.edu; khoury.neu.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"probprog_2021_nested_variational_inference.pdf (1,620,526 bytes)",1,,0,Bradley Gram-Hansen (University of Oxford); Martin Jankowiak (); Tan Zhi-Xuan (Massachusetts Institute of Technology),bradley@intelligentnetworks.ai; mjankowi@broadinstitute.org; xuan@mit.edu,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Zimmermann, Heiko*",,
20,5/6/2021 5:59:25 PM -07:00,5/12/2021 3:52:02 AM -07:00,Learning Proposals for Probabilistic Programs with Inference Combinators,"We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.",Sam Stites,stites.s@northeastern.edu,Sam Stites (Northeastern University)*; Heiko Zimmermann (Northeastern University); Hao Wu (Northeastern University); Eli Z Sennesh (Northeastern University); Jan-Willem van de Meent (Northeastern University),"Stites, Sam*; Zimmermann, Heiko; Wu, Hao; Sennesh, Eli Z; van de Meent, Jan-Willem",stites.s@northeastern.edu*; zimmermann.h@northeastern.edu; wu.hao10@northeastern.edu; esennesh@ccis.neu.edu; j.vandemeent@northeastern.edu,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,8,northeastern.edu; ccis.neu.edu,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"probprog_2021_combinators.pdf (864,104 bytes)",1,,0,,,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Syndicated Submission,Wed,,"Stites, Sam*",,
21,5/7/2021 2:11:34 AM -07:00,5/7/2021 3:28:53 AM -07:00,Variant Generation for Augmented Gibbs Samplers,"Probabilistic programming based around Gibbs samplers has not maintained usage recently because other more broadly applicable or efficient approaches exist such as variational methods and Hamiltonian Monte-Carlo (HMC).  Recent algorithms for Bayesian unsupervised learning, especially with discrete latent data (where HMC cannot be used) make use of sophisticated combinations of collapsing and augmentation with Gibbs to achieve speed and better performance, sometimes combined with hogwild Gibbs for speed.  Our research demonstrates it is possible to develop automated collapsing and augmentation with Gibbs, that could be used within a larger probabilistic programming system.
However, we add the notion of a variant because different samplers can be generated once you add a variety of choices in transforming your model.  Due to uncertainty in performance, these variants need to be tested to evaluate their comparable worth.  
Moreover, with the sampling algorithm represented internally in abstract form, it is also possible to automatically generate loop-level parallelism or hogwild Gibbs operations. We thus demonstrate automatic collapsed and augmented Gibbs with parallelism on some recent topic models.",Sachith H Seneviratne,sachith.seneviratne@unimelb.edu.au,Sachith H Seneviratne (University of Melbourne)*; Wray Buntine (Monash University),"Seneviratne, Sachith H*; Buntine, Wray",sachith.seneviratne@unimelb.edu.au*; wray.buntine@monash.edu,PROBPROG2021,"Languages, Tools, and Systems",Artificial and Natural Intelligence; Statistics and Data Analysis,1,unimelb.edu.au; monash.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"Variant_Generation_for_Augmented_Gibbs_Samplers.pdf (1,263,941 bytes)",1,,0,Carol Mak (); Kristian Kersting (TU Darmstadt); Steven Holtzen (Northeastern University),puiyiu123@gmail.com; kersting@cs.tu-darmstadt.de; s.holtzen@northeastern.edu,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Wed,,"Seneviratne, Sachith H*",,
22,5/7/2021 6:20:25 AM -07:00,5/7/2021 6:20:25 AM -07:00,A Semantics for Hybrid Probabilistic Logic Programs with Function Symbols,"Probabilistic Logic Programming (PLP) is a powerful paradigm for the representation of uncertain relations among objects. Recently, programs with continuous variables, also called hybrid programs, have been proposed and assigned a semantics. Hybrid programs are capable of representing real-world measurements but unfortunately the semantics proposal was imprecise so the definition did not assign a probability to all queries.  
In this paper, we remedy this and formally define a new semantics for hybrid programs. We prove that the semantics assigns a probability to all queries for a large class of programs.",Damiano Azzolini,damiano.azzolini@unife.it,Damiano Azzolini (University of Ferrara)*; Fabrizio Riguzzi (Universita di Ferrara); Evelina Lamma (University of Ferrara),"Azzolini, Damiano*; Riguzzi, Fabrizio; Lamma, Evelina",damiano.azzolini@unife.it*; fabrizio.riguzzi@unife.it; evelina.lamma@unife.it,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,1,unife.it,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"paper.pdf (429,380 bytes)",1,,0,,,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Azzolini, Damiano*",,
23,5/7/2021 9:35:41 AM -07:00,5/7/2021 9:35:41 AM -07:00,Compositional Semantics for Probabilistic Programs with Exact Conditioning,"This paper about probabilistic programming is awarded Distinguished Paper status at LICS 2021 (ACM/IEEE Symposium on Logic in Computer Science). In a talk or poster at ProbProg, we would focus on the aspects most suited to the ProbProg community, such as an informal discussion of the complex nature of exact inference, valid equational reasoning about programs in the presence of exact inference, and the nascent interplay between probabilistic programming and “synthetic probability theory”. The following is the abstract of our LICS paper: 

""We define a probabilistic programming language for Gaussian random variables with a first-class exact conditioning construct. We give operational, denotational and equational semantics for this language, establishing convenient properties like exchangeability of conditions. Conditioning on equality of continuous random variables is nontrivial, as the exact observation may have probability zero; this is Borel's paradox. Using categorical formulations of conditional probability, we show that the good properties of our language are not particular to Gaussians, but can be derived from universal properties, thus generalizing to wider settings. We define the Cond construction, which internalizes conditioning as a morphism, providing general compositional semantics for probabilistic programming with exact conditioning.""",Dario M Stein,dario.stein@cs.ox.ac.uk,Dario M Stein (University of Oxford)*; Sam Staton (University of Oxford),"Stein, Dario M*; Staton, Sam",dario.stein@cs.ox.ac.uk*; sam.staton@cs.ox.ac.uk,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,8,ox.ac.uk,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,Yes,"main.pdf (758,737 bytes)",1,,0,,,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Stein, Dario M*",Fri,"Stein, Dario M*"
24,5/7/2021 9:45:12 AM -07:00,5/7/2021 9:45:12 AM -07:00,Vate: Runtime Adaptable Probabilistic Programming for Java,"Inspired by earlier work on Augur, Vate is a probabilistic programming language for the construction of JVM based probabilistic models with an Object-Oriented interface. As a compiled language it is able to examine the dependency graph of the model to produce optimised code that can be dynamically targeted to different platforms. Using Gibbs Sampling, Metropolis–Hastings and variable marginalisation it can handle a range of model types and is able to efficiently infer values, estimate probabilities, and execute models.",Daniel Goodman,daniel.goodman@oracle.com,Daniel Goodman (Oracle Labs)*; Adam  C Pocock (Oracle Labs); Jason Peck (Oracle Labs); Guy Steele (Oracle Labs),"Goodman, Daniel*; Pocock, Adam  C; Peck, Jason; Steele, Guy",daniel.goodman@oracle.com*; adam.pocock@oracle.com; Jason.peck@oracle.com; Guy.Steele@oracle.com,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,3,Oracle.com;bc.edu,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"Vate paper.pdf (463,578 bytes)",1,,0,,,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Syndicated Submission,Syndicated Submission,Wed,,"Goodman, Daniel*",,
25,5/7/2021 2:37:55 PM -07:00,5/11/2021 10:56:17 AM -07:00,What do we need from a probabilistic programming language to support Bayesian workflow?,"This talk is a survey of the model building and inference steps required for a probabilistic programming language to support a pragmatic Bayesian workflow and the extent to which they're supported by existing languages. Pragmatic Bayesians eschew both of the opposing historical traditions of subjective Bayes (encode exactly your subjective beliefs as priors and turn the crank) or objective Bayes (devise ``uninformative'' priors and turn the crank).  Instead, we follow a more agile methodology which combines exploratory data analysis with exploratory model fitting, analysis, and visualization, to produce calibrated and sharp inferences for unknown quantities of interest.",Bob Carpenter,bcarpenter@flatironinstitute.org,Bob Carpenter ()*,"Carpenter, Bob*",bcarpenter@flatironinstitute.org*,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,2,flatironinstitute.org; columbia.edu ,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"carpenter-b-probprog2021.pdf (1,044,496 bytes)",1,,0,"Colin Carroll (Freebird, Inc); Neeraj Pradhan (Uber AI Labs); Viktor Senderov (Naturhistoriska Riksmuseet)",colcarroll@gmail.com; prad.neeraj@gmail.com; Viktor.Senderov@nrm.se,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Carpenter, Bob*",,
26,5/8/2021 11:21:43 PM -07:00,5/11/2021 2:37:00 PM -07:00,flip-hoisting: A Probabilistic Program Optimization for Exact Inference,"Writing fast programs is hard: the cost of the underlying computation is often unknown to the user or obscured by the system. This difficulty has led to a vast and important field of program optimizations that are applied during compilation to help users write fast code without knowing the delicate internal cost model. Optimization for probabilistic programs is still a nascent field, and here the challenge is arguably even harder than in deterministic code, since the cost of inference algorithms can be very opaque and sensitive to the exact way the code is written. In this work we give a new class of probabilistic program optimizations called flip-hoisting that is unique to discrete probabilistic programs for exact inference. This optimization seeks to reduce the overall state-space of the program by merging redundant probability distributions. We show that opportunities to apply flip-hoisting occur in practical programs and demonstrate empirically that on certain examples the overall compiled size of the program is halved.This dramatic reduction in the size of the state-space speeds up inference, sometimes by a factor of two. Long term, we see flip-hoisting as part of a standard suite of optimizations that are applied to all probabilistic programs.",Yu-Hsi Cheng,ellieyhc45@g.ucla.edu,"Yu-Hsi Cheng (University of California, Los Angeles)*; Steven J Holtzen (Northeastern University); Guy Van den Broeck (UCLA); Todd Millstein (UCLA)","Cheng, Yu-Hsi*; Holtzen, Steven J; Van den Broeck, Guy; Millstein, Todd",ellieyhc45@g.ucla.edu*; s.holtzen@northeastern.edu; guyvdb@cs.ucla.edu; todd@cs.ucla.edu,PROBPROG2021,"Languages, Tools, and Systems",,6,ucla.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,Yes,"flip-hoisting - A Probabilistic Program Optimization for Exact Inference.pdf (427,018 bytes)",1,,0,Cameron Freer (Massachusetts Institute of Technology); Christine Tasson (); Jeremy Gibbons (),freer@mit.edu; christine.tasson@irif.fr; jeremy.gibbons@cs.ox.ac.uk,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Wed,,"Cheng, Yu-Hsi*",Fri,"Cheng, Yu-Hsi*"
27,5/9/2021 12:40:52 AM -07:00,5/9/2021 12:40:52 AM -07:00,Probabilistic Programs with Stochastic Conditioning (ICML 2021),"We tackle the problem of conditioning probabilistic programs on distributions of observable variables. Probabilistic programs are usually conditioned on samples from the joint data distribution, which we refer to as deterministic conditioning. However, in many real-life scenarios, the observations are given as marginal distributions, summary statistics, or samplers. Conventional probabilistic programming systems lack adequate means for modeling and inference in such scenarios. We propose a generalization of deterministic conditioning to stochastic conditioning, that is, conditioning on the marginal distribution of a variable taking a particular form. To this end, we first define the formal notion of stochastic conditioning and discuss its key properties. We then show how to perform inference in the presence of stochastic conditioning. We demonstrate potential usage of stochastic conditioning on several case studies which involve various kinds of stochastic conditioning and are difficult to solve otherwise. Although we present stochastic conditioning in the context of probabilistic programming, our formalization is general and applicable to other settings. ",David Tolpin,david.tolpin@gmail.com,David Tolpin (Ben Gurion University of the Negev & PUB+)*; Yuan Zhou (University of Oxford); Tom Rainforth (University of Oxford); Hongseok Yang (KAIST),"Tolpin, David*; Zhou, Yuan; Rainforth, Tom; Yang, Hongseok",david.tolpin@gmail.com*; yuan.zhou@cs.ox.ac.uk; tmgr.robots@gmail.com; hongseok00@gmail.com,PROBPROG2021,Statistics and Data Analysis,"Languages, Tools, and Systems",10,ox.ac.uk; bgu.ac.il,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,No,"2010.00282.pdf (539,633 bytes)",1,,0,,,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Syndicated Submission,Syndicated Submission,Thu,,"Tolpin, David*",,
28,5/10/2021 10:17:07 AM -07:00,5/10/2021 3:07:33 PM -07:00,A Pearl Pearl,"Causal reasoning is a basic component of human intelligence. However, causal information has proven difficult for AI and machine learning applications to incorporate. This omission hinders such systems as they cannot take advantage of this important information during reasoning or explanation. Recent advances in formal causal reasoning open new possibilities to include causal reasoning in such systems. We show that causal reasoning can be represented in a Truth Maintenance Systems (TMS), and thus enable expert systems to reason causally. The integration is complete (demonstrated by the ability to answer all six ‘Firing Squad’ questions), simple (only 200 lines of Common Lisp), and practical (through an application to medical diagnosis). This paper outlines the implementation and shows how to use a TMS augmented with causal reasoning.",Nada Amin,namin@seas.harvard.edu,Nada Amin (Harvard University)*; William E Byrd (University of Alabama at Birmingham); Joseph A Cottam (PNNL); Marc-Antoine Parent (Solutions Conversence inc.); Jeremy Zucker (Pacific Northwest National Laboratory),"Amin, Nada*; Byrd, William E; Cottam, Joseph A; Parent, Marc-Antoine; Zucker, Jeremy",namin@seas.harvard.edu*; webyrd@uab.edu; Joseph.Cottam@pnnl.gov; maparent@acm.org; jeremy.zucker@pnnl.gov,PROBPROG2021,"Languages, Tools, and Systems",Artificial and Natural Intelligence; Statistics and Data Analysis; The Practice of Probabilistic Programming,2,harvard.edu; cam.ac.uk; uab.edu; pnnl.gov,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"abstract.pdf (638,697 bytes)",1,,0,David Jensen (University of Massachusetts Amherst); Wannes Meert (KU Leuven); Zenna Tavares (),jensen@cs.umass.edu; wannes.meert@kuleuven.be; zennatavares@gmail.com,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Wed,,"Amin, Nada*",,
29,5/10/2021 2:25:17 PM -07:00,5/10/2021 8:54:25 PM -07:00,Accelerating inference for InferenceQL via sum-product expressions,"Fast, exact marginalization and conditioning has recently been introduced for a broad class of probabilistic programs that can be compiled into sum-product expressions. We demonstrate that this work can be used to efficiently build and query probabilistic programs that model tabular data. The performance improvements are twofold: (i) the fast methods make developing such models more practical, and (ii) using exact inference leads to more predictable query runtime.  We present preliminary results on performance improvements in existing benchmarks for applying fast exact inference via sum-product expressions to generative population models.
",Ulrich Schaechtle,ulli@mit.edu,Ulrich Schaechtle (MIT)*; Zane Shelby (MIT); Cameron Freer (Massachusetts Institute of Technology); Feras Saad (Massachusetts Institute of Technology); Vikash Mansinghka (Massachusetts Institute of Technology),"Schaechtle, Ulrich*; Shelby, Zane; Freer, Cameron; Saad, Feras; Mansinghka, Vikash",ulli@mit.edu*; zshelby@mit.edu; freer@mit.edu; fsaad@mit.edu; vkm@mit.edu,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,9,mit.edu,3,3,100%,0,Enabled (0),Accept (Abstract),No,No,No,No,No,"main.pdf (1,869,511 bytes)",1,,0,Kristian Kersting (TU Darmstadt); Pedro Zuidberg Dos Martires (KU Leuven); Steven Holtzen (Northeastern University),kersting@cs.tu-darmstadt.de; pedro.zuidbergdosmartires@cs.kuleuven.be; s.holtzen@northeastern.edu,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Wed,,"Schaechtle, Ulrich*",,
30,5/10/2021 9:48:09 PM -07:00,5/12/2021 4:27:12 AM -07:00,Compartmental Models for COVID-19 and Control via Policy Interventions,"We demonstrate an approach to replicate and forecast the spread of the SARS-CoV-2 (COVID-19) pandemic using the toolkit of probabilistic programming languages (PPLs). Our goal is to study the impact of various modeling assumptions and motivate policy interventions enacted to limit the spread of infectious diseases. Using existing compartmental models we show how to use inference in PPLs to obtain posterior estimates for disease parameters. We improve popular existing models to reflect practical considerations such as the under-reporting of the true number of COVID-19 cases and motivate the need to model policy interventions for real-world data.  We design an SEI3RD model as a reusable template and demonstrate its flexibility in comparison to other models. We also provide a greedy algorithm that selects the optimal series of policy interventions that are likely to control the infected population subject to provided constraints. We work within a simple, modular, and reproducible framework to enable immediate cross-domain access to the state-of-the-art in probabilistic inference with emphasis on policy interventions. \textbf{We are not epidemiologists}; the sole aim of this study is to serve as a exposition of methods, not to directly infer the real-world impact of policy-making for COVID-19.",Swapneel S Mehta,swapneel.mehta@nyu.edu,Swapneel S Mehta (New York University)*; Noah S Kasmanoff (New York University),"Mehta, Swapneel S*; Kasmanoff, Noah S",swapneel.mehta@nyu.edu*; nsk367@nyu.edu,PROBPROG2021,The Practice of Probabilistic Programming,Statistics and Data Analysis,0,nyu.edu,3,3,100%,0,Enabled (4),Accept (Abstract),No,No,No,No,No,"PROBPROG__Final_Submission___COVID_Modeling.pdf (2,102,887 bytes)",1,,0,Christian Weilbach (University of British Columbia); David Tolpin (Ben Gurion University of the Negev & PUB+); Heiko Zimmermann (Northeastern University),ch_weil@topiq.es; david.tolpin@gmail.com; hzimmermann@ccs.neu.edu,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Mehta, Swapneel S*",,
31,5/11/2021 12:44:03 AM -07:00,5/11/2021 12:44:03 AM -07:00,Efficient Generative Modelling of Protein StructureFragments using a Deep Markov Model,"Fragment libraries are often used in protein structure prediction, simulation and design as a means to significantly reduce the vast conformational search space.
Current state-of-the-art methods for fragment library generation do not properly account for aleatory and epistemic uncertainty, respectively due to the dynamic nature of proteins and experimental errors and noise in protein structures. 
They also typically rely on information that is not generally or readily available, such as homologous sequences, related protein structures or other complementary information.
To address these issues, we developed BIFROST, a novel take on the fragment library problem based on a Deep Markov Model architecture combined with directional statistics for the angular degrees of freedom, implemented in the deep probabilistic programming language Pyro.
BIFROST is a probabilistic, generative model of the protein backbone dihedral angles conditioned solely on the amino acid sequence.
BIFROST generates fragment libraries with a quality on par with current state-of-the-art methods at a fraction of the run-time, while requiring considerably less information and allowing efficient evaluation of probabilities.",Christian B Thygesen,ckt@evaxion-biotech.com,Christian B Thygesen (University of Copenhagen / Evaxion Biotech)*; Ahmad Salim Al-Sibahi (University of Copenhagen); Christian Skjødt Steenmans (Evaxion Biotech); Lys Sanz Moreta (University of Copenhagen); Anders Bundgård Sørensen (Evaxion Biotech); Thomas Hamelryck (University of Copenhagen),"Thygesen, Christian B*; Al-Sibahi, Ahmad Salim; Skjødt Steenmans, Christian; Sanz Moreta, Lys; Bundgård Sørensen, Anders; Hamelryck, Thomas",ckt@evaxion-biotech.com*; ahmad@di.ku.dk; csh@evaxion-biotech.com; moreta@di.ku.dk; abs@evaxion-biotech.com; thamelry@binf.ku.dk,PROBPROG2021,The Practice of Probabilistic Programming,Statistics and Data Analysis,0,evaxion-biotech.com; di.ku.dk; bio.ku.dk; binf.ku.dk,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,Yes,"Efficient_Generative_Modelling_of_Protein_Structure_Fragments_using_a_Deep_Markov_Model___ICML_2021 (2).pdf (1,455,961 bytes)",1,,0,,,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Thygesen, Christian B*",Thu,"Thygesen, Christian B*"
32,5/11/2021 4:09:53 AM -07:00,5/11/2021 5:47:18 AM -07:00,Expectation Programming,"Building on ideas from probabilistic programming, we introduce the concept of an expectation programming framework (EPF) that automates the calculation of expectations. Analogous to a probabilistic program, an expectation program is comprised of a mix of probabilistic constructs and deterministic calculations that define a conditional distribution over its variables. Inference is then required to draw samples from this distribution. However, the focus of the inference engine in an EPF is to directly estimate the resulting expectation of the program return values, rather than approximate the conditional distribution itself. This is made possible by exploiting recent advancements in target-aware Bayesian inference to tailor our inference engines to this expectation estimation, providing the potential for substantial improvements over the standard probabilistic programming pipeline. We realize a particular instantiation of our EPF concept by extending the probabilistic programming language Turing with a new macro that uses a series of program transformations to automatically run target-aware inference. We show that this leads to significant empirical gains in estimation performance compared to conventional use of Turing on three different example problems. ",Tim Reichelt,reichelt@robots.ox.ac.uk,Tim Reichelt (University of Oxford)*; Adam Golinski (University of Oxford); Luke Ong (University of Oxford); Tom Rainforth (University of Oxford),"Reichelt, Tim*; Golinski, Adam; Ong, Luke; Rainforth, Tom",reichelt@robots.ox.ac.uk*; adamg@robots.ox.ac.uk; lo@cs.ox.ac.uk; rainforth@stats.ox.ac.uk,PROBPROG2021,Statistics and Data Analysis,The Practice of Probabilistic Programming,9,ox.ac.uk; qualcomm.com; bosch-ai.com,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"probprog-2021.pdf (1,522,068 bytes)",1,,0,"Gordon Plotkin (); Sam Witty (University of Massachusetts, Amherst); Wray Buntine (Monash University)",plotkin@google.com; switty@cs.umass.edu; wray.buntine@monash.edu,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Thu,,"Reichelt, Tim*",,
33,5/11/2021 9:20:06 AM -07:00,5/12/2021 2:54:48 AM -07:00,Nonparametric Hamiltonian Monte Carlo,"Probabilistic programming uses programs to express generative models whose posterior probability is then computed by built-in inference engines. A challenging goal is to develop general purpose inference algorithms that work-out-of-the-box for arbitrary programs in a universal probabilistic programming language (PPL). The densities defined by such programs, which may use (stochastic) branching and recursion, are (in general) nonparametric, in the sense that they correspond to models on an infinite-dimensional parameter space. However standard inference algorithms, such as the Hamiltonian Monte Carlo (HMC) algorithm, target distributions with a fixed number of parameters. This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC) algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a new class of measurable functions called “tree representable”, which serve as a language-independent representation of the density functions of probabilistic programs in a universal PPL. We provide a correctness proof of NP-HMC, and empirically demonstrate significant performance improvements over existing approaches on several nonparametric examples.",Carol Mak,pui.mak@cs.ox.ac.uk,Carol Mak (University of Oxford)*; Fabian Zaiser (University of Oxford); Luke Ong (University of Oxford),"Mak, Carol*; Zaiser, Fabian; Ong, Luke",pui.mak@cs.ox.ac.uk*; fabian.zaiser@cs.ox.ac.uk; luke.ong@cs.ox.ac.uk,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,8,ox.ac.uk; uni-bonn.de,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,Yes,"main.pdf (1,299,373 bytes)",1,,0,,,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Syndicated Submission,Syndicated Submission,Thu,,"Mak, Carol*",Fri,"Mak, Carol*"
34,5/11/2021 10:55:09 AM -07:00,5/11/2021 10:55:09 AM -07:00,Probabilistic Programming for Bond Trading,"Machine-assisted trading of fixed income securities, or bonds, is a challenging problem. We present rationale and preliminary results in support of using probabilistic programming for this application, and demonstrate a query for few-shot-learning-based search of bond trades implemented in a probabilistic programming system (PPS). Additionally, we introduce a novel algorithm for active learning implemented in a PPS that can help traders find bonds that match their chosen strategy. Initial experimental results are provided with simulated data to show this algorithm has the potential to increase search efficiency compared with non-active alternatives.",Jameson A Quinn,jameson.quinn@gmail.com,Jameson A Quinn (Jameson Quinn)*; Veronica Weiner (MIT); Harish Tella (MIT),"Quinn, Jameson A*; Weiner, Veronica; Tella, Harish",jameson.quinn@gmail.com*; vsw@mit.edu; htella@mit.edu,PROBPROG2021,The Practice of Probabilistic Programming,,7,mit.edu; harvard.edu,3,3,100%,0,Enabled (2),Accept (Abstract),No,No,No,No,No,"Probabilistic Programming for Bond Trading.pdf (1,240,238 bytes)",1,,0,"Colin Carroll (Freebird, Inc); David Broman (); Nimar Arora (Bayesian Logic)",colcarroll@gmail.com; dbro@kth.se; nimar.arora@gmail.com,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Quinn, Jameson A*",,
35,5/11/2021 1:19:34 PM -07:00,5/11/2021 1:19:34 PM -07:00,Variational Energy Conserving Subsampling,"Efficient and accurate Bayesian inference using Markov Chain Monte Carlo methods in the big data caseis an open challenge. Recently, [1] proposed Energy Conserving Subsampling (ECS) for Bayesian inferencewith Hamiltonian Monte Carlo (HMC) on subsets of data. However, their method for subsampling requirescomputing a memory intensive control variate based on a second order Taylor expansion. To avoid this, wepropose a novel control variate we call Variational ECS (VECS). VECS uses an approximate posterior forlikelihood estimation, thus preventing the quadratic expansion of feature space. However, VECS currentlyfails to effectively incorporate the state of the Markov chain. We present a possible work around, and invitediscussion for other options. We implemented VECS in the probabilistic programming language NumPyro[2], which allows us to leverage efficient implementations of ECS, Stochastic Variational Inference, andHamiltonian Monte Carlo.",Ola Rønning,ola@di.ku.dk,Ola Rønning (University of Copenhagen)*; Thomas Hamelryck (University of Copenhagen),"Rønning, Ola*; Hamelryck, Thomas",ola@di.ku.dk*; thamelry@bio.ku.dk,PROBPROG2021,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,0,ku.dk,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"Variational_Energy_Conserving_Subsampling.pdf (739,001 bytes)",1,,0,Eli Sennesh (Northeastern University); Neeraj Pradhan (Uber AI Labs); Theofanis Karaletsos (Facebook),esennesh@ccs.neu.edu; prad.neeraj@gmail.com; theofanis.karaletsos@gmail.com,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Thu,,"Rønning, Ola*",,
36,5/11/2021 2:16:31 PM -07:00,5/11/2021 2:16:31 PM -07:00,Autocorrect for Probabilistic Models,"Probabilistic modelling has become easier than ever to apply to a
myriad of problems due to the development of modern approximate
inference algorithms. Unfortunately, model checking and model
selection continues to be a challenging ad-hoc process. We introduce a
method for suggesting improvements to a probabilistic model which are
then supported by a relative goodness of fit test. We require
relatively little of the models hence making it applicable in many
places. We don't even require a density from the models allowing it to
be used as a fairly blackbox method.
",Robert Zinkov,zinkov@robots.ox.ac.uk,Robert Zinkov (University of Oxford)*,"Zinkov, Robert*",zinkov@robots.ox.ac.uk*,PROBPROG2021,The Practice of Probabilistic Programming,"Languages, Tools, and Systems",11,ox.ac.uk;iu.edu;ubc.ca,3,3,100%,0,Enabled (0),Accept (Abstract),No,No,No,No,No,"paper.pdf (332,272 bytes)",1,,0,Benjamin Sherman (MIT); Eli Bingham (); Hao Wu (),sherman@csail.mit.edu; ebingham@broadinstitute.org; haowu@ccis.neu.edu,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Thu,,"Zinkov, Robert*",,
37,5/11/2021 7:06:49 PM -07:00,5/11/2021 7:06:49 PM -07:00,Explorations of causal probabilistic programming approaches for rule-based models of biological signaling pathways,"Signaling pathways are the molecular circuits that underlie cellular processes, such as proliferation and cell-cell communication. Recently, signaling pathways have been represented using rule-based models, where agents and their interactions are viewed as events that cause the system to change to a new state. In many cases, we are interested in counterfactual queries of signaling pathways, where the chance of the occurrence of an event is defined not only by what happened, but also by what did not happen. We argue that probabilistic programming languages (PPL) offer a robust framework for deriving such counterfactual traces in rule-based models of signaling pathways. We explore for this task Omega, a causal probabilistic programming language, probability trees, which can represent a causal generative or stochastic process, and Kappa, a language for rule-based modeling. We highlight advantages, limitations, and practical implementation challenges of each language for this application.",Devon Kohler,kohler.d@northeastern.edu,Devon Kohler (Northeastern University)*; Jeremy Zucker (Pacific Northwest National Laboratory); Vartika Tewari (Northeastern University ); Karen Sachs (Next Generation Analytics); Robert Ness (Altdeep.ai); Olga Vitek (Northeastern University),"Kohler, Devon*; Zucker, Jeremy; Tewari, Vartika; Sachs, Karen; Ness, Robert; Vitek, Olga",kohler.d@northeastern.edu*; jeremy.zucker@pnnl.gov; tewari.v@northeastern.edu; sachskaren@gmail.com; robert@altdeep.ai; o.vitek@northeastern.edu,PROBPROG2021,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",9,northeastern.edu; pnnl.gov; altdeep.ai; nextgenanalytics.us; mit.edu,3,3,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"Kohler_Zucker_Tewari_Sachs_Ness_Vitek_Abstract.pdf (765,994 bytes)",1,,0,Javier Burroni (UMass Amherst); Yura Perov (EQL); Zenna Tavares (),javier.burroni@gmail.com; yura.perov@gmail.com; zennatavares@gmail.com,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Kohler, Devon*",,
39,5/11/2021 9:05:39 PM -07:00,5/12/2021 2:22:18 AM -07:00,Recursive Monte Carlo and Variational Inference,"Many complex Monte Carlo and variational inference algorithms can be justified as standard importance sampling or variational optimization on an extended space. But such constructions can be difficult to reason about: what principles guide the design of extended state spaces for auxiliary-variable algorithms? We present Recursive Auxiliary-Variable Inference (RAVI), a framework for understanding, designing, and improving auxiliary-variable Monte Carlo and variational inference algorithms. RAVI decomposes an algorithm into a stack of approximations, which can each be designed or analyzed individually. We present: (1) theorems guiding the design of these stacks; (2) 10 examples of existing algorithms, decomposed as stacks; and (3) an example of improving an algorithm by optimizing its stack. We also introduce RAVI programmable inference for PPLs: if a stack is encoded as a sequence of probabilistic programs, its validity can be tested automatically, and the implementation of the corresponding Monte Carlo and variational inference algorithms can be automated.",Alexander K. Lew,alexlew@mit.edu,Alexander K. Lew (MIT)*; Marco Cusumano-Towner (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Lew, Alexander K.*; Cusumano-Towner, Marco; Mansinghka, Vikash",alexlew@mit.edu*; marcoct@mit.edu; vkm@mit.edu,PROBPROG2021,Statistics and Data Analysis,"Languages, Tools, and Systems",7,mit.edu;,3,3,100%,0,Enabled (4),Accept (Abstract),No,No,No,No,No,"probprog21_recursive-9.pdf (999,154 bytes)",1,,0,Daniel Roy (University of Toronto); Eli Sennesh (Northeastern University); Heiko Zimmermann (Northeastern University),daniel.roy@utoronto.ca; esennesh@ccs.neu.edu; hzimmermann@ccs.neu.edu,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Lew, Alexander K.*",,
40,5/11/2021 10:25:46 PM -07:00,5/11/2021 10:25:46 PM -07:00,Causal Probabilistic Programming Without Tears,"We take an informal, example-driven tour of some of the underlying questions that causal inference researchers and practitioners seek to answer, and the causal assumptions that make it possible to derive these answers from data. Taken together with other recent literature, our examples point toward an emerging research agenda under which many conceptual and practical difficulties in applying causal inference methods could be alleviated by framing causal questions as source code transformations and standard probabilistic computations on causal models instantiated as programs in a modern generative probabilistic programming language.",Eli Bingham,eli@elibingham.com,"Eli Bingham (Broad Institute of MIT and Harvard)*; James Koppel (MIT); Alexander K. Lew (MIT); Robert Ness (Altdeep); Zenna Tavares (MIT); Sam A Witty (University of Massachusetts, Amherst); Jeremy Zucker (Pacific Northwest National Laboratory)","Bingham, Eli*; Koppel, James; Lew, Alexander K.; Ness, Robert; Tavares, Zenna; Witty, Sam A; Zucker, Jeremy",eli@elibingham.com*; jkoppel@mit.edu; alexlew@mit.edu; robert@altdeep.ai; zt2297@columbia.edu; switty@cs.umass.edu; jeremy.zucker@pnnl.gov,PROBPROG2021,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",13,broadinstitute.org; mit.edu; pnnl.gov; northeastern.edu; uber.com; umass.edu; altdeep.ai,2,2,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,Yes,"probprog21_causality(1).pdf (680,771 bytes)",1,,0,Todd Millstein (UCLA); Yura Perov (EQL),todd@cs.ucla.edu; yura.perov@gmail.com,Jan-Willem van de Meent (Northeastern University),j.vandemeent@northeastern.edu,,,Extended Abstract,Extended Abstract,Wed,,"Bingham, Eli*",Wed,"Bingham, Eli*"
41,5/11/2021 11:29:25 PM -07:00,5/11/2021 11:29:25 PM -07:00,Propagating Gradients through Weights in Particle Filters,"We develop a method for differentiating through the standard resampling step used in particle filters. It yields unbiased estimators for the gradient of the marginal likelihood by propagating the gradients through the particle weights. By careful use of the stop-gradient operator, our method can be added to a particle filter in a few lines of code while exactly preserving the results of the forward pass. The computational overhead is negligible, the backward pass is fully automated by standard automatic differentiation libraries, and we achieve significant improvements over the alternatives in preliminary experiments.",Adam Scibior,ascibior@cs.ubc.ca,Adam Scibior (University of British Columbia)*; Vaden W Masrani (University of British Columbia); Frank Wood (University of British Columbia),"Scibior, Adam*; Masrani, Vaden W; Wood, Frank",ascibior@cs.ubc.ca*; vadmas@cs.ubc.ca; fwood@cs.ubc.ca,PROBPROG2021,Statistics and Data Analysis,"Languages, Tools, and Systems",5,ubc.ca; cam.ac.uk,2,2,100%,0,Enabled (1),Accept (Abstract),No,No,No,No,No,"PROBPROG2021_PF.pdf (600,757 bytes)",1,,0,"Sam Staton (University of Oxford); Sam Witty (University of Massachusetts, Amherst)",sam.staton@cs.ox.ac.uk; switty@cs.umass.edu,Lawrence Murray (Uber AI),lawrence.murray@uber.com,,,Extended Abstract,Extended Abstract,Wed,,"Scibior, Adam*",,
42,5/12/2021 2:20:10 AM -07:00,5/12/2021 2:20:10 AM -07:00,Bayesian strategies: higher-order probabilistic programs as graphical models ,"We introduce Bayesian strategies, a new interpretation of
probabilistic programs in game semantics. This interpretation can be
seen as a refinement of Bayesian networks.
Bayesian strategies are based on a new form of event structure, with two
causal dependency relations respectively modelling control flow and data
flow. This gives a graphical representation for probabilistic programs
which resembles the concrete representations used in modern implementations of probabilistic programming.
From a theoretical viewpoint, Bayesian strategies provide a rich setting
for denotational semantics. To demonstrate this we give a model for a
general higher-order programming language with recursion, conditional
statements, and primitives for sampling from continuous distributions
and trace re-weighting. This is significant because Bayesian networks do
not easily support higher-order functions or conditionals.",Hugo Paquet,hugo.paquet@cs.ox.ac.uk,Hugo Paquet (University of Oxford)*,"Paquet, Hugo*",hugo.paquet@cs.ox.ac.uk*,PROBPROG2021,"Languages, Tools, and Systems",,8,ox.ac.uk;,0,0,0%,0,Enabled (0),Accept (Syndicated),No,No,No,No,Yes,"esop21.pdf (531,633 bytes)",1,,0,,,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Syndicated Submission,Syndicated Submission,Thu,,"Paquet, Hugo*",Thu,"Paquet, Hugo*"
43,5/12/2021 2:44:00 AM -07:00,5/12/2021 4:59:04 AM -07:00,LazyPPL: Sampling-by-need in non-parametric probabilistic programs,"We introduce a prototype probabilistic programming library called LazyPPL which is focused on clear implementations of non-parametric models by using lazy evaluation. Haskell's laziness is sometimes called `call-by-need': you only run the bits of programs that are needed for the result. In probabilistic programming, this becomes `sample-by-need': you only sample from distributions that are needed for the result. We illustrate the power of the language with natural specifications of infinite structures including Poisson point processes, Gaussian processes, and Dirichlet Process clustering, and a simple Metropolis-Hastings implementation.",Sam Staton,sam.staton@cs.ox.ac.uk,Sam Staton (University of Oxford)*,"Staton, Sam*",sam.staton@cs.ox.ac.uk*,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis,8,ox.ac.uk,3,3,100%,0,Enabled (3),Accept (Abstract),No,No,No,No,No,"lazyppl.pdf (891,022 bytes)",1,,0,Daniel Roy (University of Toronto); Gordon Plotkin (); Michele Pagani (),daniel.roy@utoronto.ca; plotkin@google.com; pagani@irif.fr,Jean-Baptiste Tristan (Boston College),tristanj@bc.edu,,,Extended Abstract,Extended Abstract,Thu,,"Staton, Sam*",,
44,5/12/2021 3:06:01 AM -07:00,5/12/2021 3:23:05 AM -07:00,Parallel Variable Elimination in Dynamic Factor Graphs,"Factor graphs provide a flexible representation of joint probability distributions over discrete variables.
Their great generality, however, obscures repeated structure that, if present, is crucial for enabling efficient computation of 
fundamental quantities like the marginal log likelihood. 
To exploit such structure we introduce the notion of a dynamic plated factor graph, which generalizes plated factor graphs to include overlapping tiles of a basic template. 
To enable their use as basic modeling components that can be deployed in a wide variety of contexts, we express
dynamic plated factor graphs as compact probabilistic programs in Pyro. 
We show that the resulting family of factor graphs admits highly-parallelizable and fast inference algorithms, and
in an empirical evaluation we demonstrate the speed-ups that result from fully exploiting repeated structure. ",Eli Bingham,eli@elibingham.com,Eli Bingham (Broad Institute of MIT and Harvard)*; Fritz Obermeyer (); Yerdos Ordabayev (Brandeis University); Du Phan (University of Illinois at Urbana-Champaign); Martin Jankowiak (),"Bingham, Eli*; Obermeyer, Fritz; Ordabayev, Yerdos; Phan, Du; Jankowiak, Martin",eli@elibingham.com*; fobermey@broadinstitute.org; ordabayev@brandeis.edu; phandu@illinois.edu; mjankowi@broadinstitute.org,PROBPROG2021,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,4,broadinstitute.org; uber.com; illinois.edu; brandeis.edu; ,3,3,100%,0,Enabled (0),Accept (Abstract),No,No,No,No,No,"probprog21_tve(5).pdf (1,568,189 bytes)",1,,0,"Carol Mak (); Wannes Meert (KU Leuven); Yuan Zhou (Artificial Intelligence Research Center, DII)",puiyiu123@gmail.com; wannes.meert@kuleuven.be; yuaanzhou@outlook.com,Guy Van den Broeck (UCLA),guyvdb@cs.ucla.edu,,,Extended Abstract,Extended Abstract,Wed,,"Bingham, Eli*",,