ID,Created,Last Modified,Title,Abstract,Primary Contact Author Name,Primary Contact Author Email,Authors,Author Names,Author Emails,Giving Talk?,Poster,Floor,Attendee registered,Talk,Primary Subject Area,Secondary Subject Areas,Type,DECISION,PRESENTER
3,1/4/2020 8:37:23 AM -08:00,1/10/2020 6:52:46 AM -08:00,Hierarchical Modelling for High Throughput Protein Engineering,"We submit a case study showcasing the use of hierarchical Bayesian statistical modelling in high throughput measurements of enzyme activity for protein engineering purposes. High throughput biological measurements are noisy, have variable numbers of replicate measurements per sample, and involve large numbers of samples. We use hierarchical Bayesian models with approximate inference methods to address all three issues. High throughput protein engineering also focuses on two decision points: which samples to re-measure, and which samples to prioritize, with one goal: find the best extreme-valued sample(s). Uncertainty features heavily in both decisions, and we show how this is accomplished.",,,Eric ma (Novartis Institutes for Biomedical Research)*; Arkadij Kummer (Novartis Institutes for Biomedical Research ); Richard Lewis (Novartis Institutes for Biomedical Research ),"ma, Eric*; Kummer, Arkadij; Lewis, Richard",ericmajinglong@gmail.com*; arkadij.kummer@novartis.com; richard.lewis@novartis.com,,Fri,1,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,Arkadij Kummer
4,1/4/2020 12:35:48 PM -08:00,1/11/2020 12:19:51 AM -08:00,Effective Monte Carlo Variational Inference for Binary-Variable Probabilistic Programs,"We propose a broadly applicable variational inference algorithm for probabilistic models with binary latent variables, using sampling to approximate expectations required for coordinate ascent updates. Applied to three real-world models for text and image and network data, our approach converges much faster than REINFORCE-style stochastic gradient algorithms, and requires fewer Monte Carlo samples. Compared to hand-crafted variational bounds with model-dependent auxiliary variables, our approach leads to tighter likelihood bounds and greater robustness to local optima. Our method is designed to integrate easily with probabilistic programming languages for effective, scalable, black-box variational inference.",,,"Geng Ji (University of California, Irvine)*; Erik B Sudderth (University of California, Irvine)","Ji, Geng*; Sudderth, Erik B",gji1@uci.edu*; sudderth@uci.edu,,Fri,1,,,Artificial and Natural Intelligence,Statistics and Data Analysis; The Practice of Probabilistic Programming,Extended Abstract,Accept,
6,1/6/2020 6:25:59 AM -08:00,1/9/2020 6:40:21 PM -08:00,Soss: Declarative Probabilistic Programming via Runtime Code Generation,"We present Soss, a declarative probabilistic programming language embedded in the Julia language. Soss represents statistical models in terms of abstract syntax trees, and uses staged compilation for on-demand generation of ""inference primitives"" (random sampling, log-density, etc) without requiring casual users to worry about such details.  The approach taken by Soss makes it easy to extend to take advantage of other packages in the rapidly-growing Julia ecosystem. At the time of this writing, Soss users can choose from several inference back-ends and connect easily with larger systems SymPy and Gen.",,,Chad Scherrer (RelationalAI)*; Taine Zhao (University of Tsukuba),"Scherrer, Chad*; Zhao, Taine",chad.scherrer@gmail.com*; thaut@logic.cs.tsukuba.ac.jp,,Thu,1,,,The Practice of Probabilistic Programming,"Artificial and Natural Intelligence; Languages, Tools, and Systems; Statistics and Data Analysis",Extended Abstract,Accept,
7,1/7/2020 3:30:33 AM -08:00,1/7/2020 3:30:33 AM -08:00,A Probabilistic Programming Approach to Protein Structure Superposition,"Optimal superposition of protein structures is crucial for understanding their structure, function, dynamics and evolution. We investigate the use of probabilistic programming to superimpose protein structures guided by a Bayesian model. Our model THESEUS-PP is based on the THESEUS model, a probabilistic model of protein superposition based on rotation, translation and perturbation of an underlying, latent mean structure. The model was implemented in the deep probabilistic programming language Pyro. Unlike conventional methods that minimize the sum of the squared distances, THESEUS takes into account correlated atom positions and heteroscedasticity (i.e., atom positions can feature different variances). THESEUS performs maximum likelihood estimation using iterative expectation-maximization. In contrast, THESEUS-PP allows automated maximum a-posteriori (MAP) estimation using suitable priors over rotation, translation, variances and latent mean structure. The results indicate that probabilistic programming is a powerful new paradigm for the formulation of Bayesian probabilistic models concerning biomolecular structure. Specifically, we envision the use of the THESEUS-PP model as a suitable error model or likelihood in Bayesian protein structure prediction using deep probabilistic programming.",,,Lys Sanz Moreta (University of Copenhagen)*; Thomas Hamelryck (University of Copenhagen); Ahmad Salim Al-Sibahi (University of Copenhagen/Skanned),"Sanz Moreta, Lys*; Hamelryck, Thomas; Al-Sibahi, Ahmad Salim",moreta@di.ku.dk*; thamelry@binf.ku.dk; ahmad@di.ku.dk,,Fri,1,,,The Practice of Probabilistic Programming,Protein Superposition,Syndicated Submission,Accept,
8,1/8/2020 2:46:28 AM -08:00,1/10/2020 1:50:19 AM -08:00,MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic Programming,"We elaborate on using importance sampling for causal reasoning, in particular for counterfactual inference. We show how this can be implemented natively in probabilistic programming. By considering the structure of the counterfactual query, one can significantly optimise the inference process. We also consider design choices to enable further optimisations. We introduce MultiVerse, a probabilistic programming prototype engine for approximate causal reasoning. We provide experimental results and compare with Pyro, an existing probabilistic programming framework with some of causal reasoning tools.  Notes for the submission (not part of the abstract): (1) this paper was previously submitted, accepted to and presented at the Second Symposium on Advances in Approximate Bayesian Inference (December 2019); (2) we would like an extended version of our work (with more focus on applications of our approach to models i.e. with new significant contributions) to be potentially considered for the Probabilistic Programming journal (depending on the deadlines of the journal submission and if it is appropriate to submit an extended version with new results); note that the current version has been already accepted for the Proceedings of Machine Learning Research as part of the Symposium.",,,"Yura Perov (Babylon Health)*; Yura Perov (Babylon Health); Logan Graham (University of Oxford); Kostis Gourgoulias (Babylon Health); Jon Richens (Babylon Health); Ciarán Lee (Babylon Health, UCL); Adam Baker (Babylon Health); Saurabh Johri (Babylon Health)","Perov, Yura*; Perov, Yura; Graham, Logan; Gourgoulias, Kostis; Richens, Jon; Lee, Ciarán; Baker, Adam; Johri, Saurabh",yuraperov@gmail.com*; yura.perov@babylonhealth.com; logan@robots.ox.ac.uk; kostis.gourgoulias@babylonhealth.com; jonathan.richens@babylonhealth.com; ciaran.lee@babylonhealth.com; adam.baker@babylonhealth.com; saurabh.johri@babylonhealth.com,Yes,Fri,1,,Fri,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,Syndicated Submission,Talk,"Graham Logan presenting poster, Yura Perov presenting talk"
9,1/8/2020 7:33:22 AM -08:00,1/9/2020 10:16:19 AM -08:00,Programming Reactive Probabilistic Applications,"Synchronous languages were introduced for the design of real-time embedded systems. These dedicated languages allow you to write a precise specification of the system, simulate it, validate it with tests or formal verification and then compile it to executable code. However, they offer limited support for modeling the non-deterministic behaviors that are ubiquitous in embedded systems.  In this paper, we present Probzelus, a probabilistic extension of a synchronous language descending from Lustre. Probzelus allows to describe reactive probabilistic models interacting with an observable environment. At run time, a set of inference techniques can be used to learn model parameter distributions from observed data. We illustrate the expressiveness of Probzelus with examples such as a trajectory detector from noisy observations, or a robot controller able to infer both its position and a map of its environment.",,,Guillaume Baudart (IBM Research)*; Louis Mandel (IBM Research); Marc Pouzet (ENS); Eric Atkinson (MIT); Benjamin Sherman (MIT); Michael Carbin (MIT),"Baudart, Guillaume*; Mandel, Louis; Pouzet, Marc; Atkinson, Eric; Sherman, Benjamin; Carbin, Michael",guillaume.baudart@ibm.com*; lmandel@us.ibm.com; marc.pouzet@ens.fr; eatkinson@csail.mit.edu; sherman@csail.mit.edu; mcarbin@csail.mit.edu,,Fri,1,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
11,1/8/2020 9:31:58 AM -08:00,1/8/2020 9:31:58 AM -08:00,Deployable probabilistic programming,"We propose design guidelines for a probabilistic programming facility suitable for deployment as a part of a production software system. As a reference implementation, we introduce Infergo, a probabilistic programming facility for Go, a modern programming language of choice for server-side software development. We argue that a similar probabilistic programming facility can be added to most modern general-purpose programming languages.  Probabilistic programming enables automatic tuning of program parameters and algorithmic decision making through probabilistic inference based on the data. To facilitate addition of probabilistic programming capabilities to other programming languages, we share implementation choices and techniques employed in development of Infergo. We illustrate applicability of Infergo to various use cases on case studies, and evaluate Infergo's performance on several benchmarks, comparing Infergo to dedicated inference-centric probabilistic programming frameworks. ",,,David Tolpin (Ben Gurion University of the Negev & PUB+)*,"Tolpin, David*",david.tolpin@gmail.com*,,Fri,1,,,"Languages, Tools, and Systems",,Syndicated Submission,Accept,
12,1/9/2020 2:47:38 AM -08:00,1/9/2020 2:47:38 AM -08:00,Approximations in Probabilistic Programs: a Compositional Nonasymptotic analysis of Nested MCMC,"We study the first-order probabilistic programming language introduced by Staton et al.~ [2016], but with an additional language construct, \textbf{stat}, that, like the fixpoint operator of Atkinson et al.~[2018], converts the description of the Markov kernel of an ergodic Markov chain into a sample from its unique stationary distribution. Up to minor changes in how certain error conditions are handled, we show that \textbf{norm} and \textbf{score} are eliminable from the extended language. We then explore the problem of approximately implementing the semantics of the language with potentially nested \textbf{stat} expressions, in a language without \textbf{stat}. For a single \textbf{stat} term, the error introduced by the finite unrolling proposed by Atkinson et al.~ [2018] vanishes only asymptotically. In the general case, no guarantees exist. Under uniform ergodicity assumptions, we are able to give quantitative error bounds and convergence results for the approximate implementation of the extended first-order language. ",,,Ekansh Sharma (University of Toronto)*; Daniel M. Roy (University of Toronto),"Sharma, Ekansh*; Roy, Daniel M.",ekansh@cs.toronto.edu*; droy@utstat.toronto.edu,Yes,Thu,1,,Sat,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Talk,
13,1/9/2020 12:55:45 PM -08:00,1/10/2020 3:38:50 PM -08:00,Automated statistical tests for probabilistic programs,"The objective of goodness-of-fit (GOF) testing is to assess whether a dataset of observations is likely to have been drawn from a candidate probability distribution. GOF testing is fundamental to several tasks that arise in probabilistic programming: (1) unit tests, to ensure the accuracy with which the distributions for probabilistic primitives have been implemented; (2) checks of the quality of posterior inference, including cases where we do not know a closed form for the posterior, and may not even be able to generate exact samples from it; (3) predictive posterior checks to ensure model fit, where the model is a probabilistic program that has been learned from a dataset.  This paper presents a family of GOF tests that are simulation-based, and crucially do not depend on knowing the density or CDF in closed form. The method also admits functional tests, in that the user can specify certain aspects of the distribution that of interest for a given application, and customize the test to these derived random variables.  The basic technique is directly applicable to unit testing of PPSs as in (1). Towards checking posterior inference as in (2), we illustrate (using random partitions in Dirichlet process mixture models) how the test can be used to extend Simulation-Based Calibration so as to verify the accuracy of PPS inference even when exact samples are not possible. We also illustrate how to estimate the quality of posterior inference (2) by using the test to compare with a small number of exact samples of a random lattice in an Ising model.  Towards (3), we aim to develop future applications of this test to diagnose the fit of models learned from Crosscat and other methods for learning distributions from data.  [This work appeared in AISTATS 2019, on April 16-18, 2019: A Family of Exact Goodness-of-Fit Tests for High-Dimensional Discrete Distributions. Feras A. Saad, Cameron E. Freer, Nathanael L. Ackerman, Vikash K. Mansinghka. PMLR 89:1640-1649, 2019.]",,,Feras Saad (Massachusetts Institute of Technology); Cameron Freer (Massachusetts Institute of Technology)*; Nate Ackerman (Harvard University); Vikash Mansinghka (Massachusetts Institute of Technology),"Saad, Feras; Freer, Cameron*; Ackerman, Nate; Mansinghka, Vikash",fsaad@mit.edu; freer@mit.edu*; nate@math.harvard.edu; vkm@mit.edu,,Fri,1,,,Statistics and Data Analysis,"Languages, Tools, and Systems; The Practice of Probabilistic Programming",Syndicated Submission,Accept,
14,1/9/2020 1:11:16 PM -08:00,1/10/2020 7:19:20 PM -08:00,Optimal approximate sampling for probabilistic programming,"Probabilistic modeling often makes use of relatively few random bits -- but how few, and what are the tradeoffs?  Given a specified bit precision, what is the most effective way to make use of this budget, and can programs run faster or use less memory when run with less randomness?  This paper addresses these questions by determining the most accurate and entropy-efficient algorithms for approximately sampling from a given discrete probability distribution, where the probabilities of the output distribution must be specified using at most a given number of bits of precision. The notion of approximation can be an arbitrary f-divergence (not just total variation distance). In a wide range of circumstances we demonstrate that these algorithms are superior in accuracy, entropy consumption, needed precision, and/or wall-clock runtime to existing approximate samplers, and establish that they often consume significantly fewer resources than are needed by exact samplers. We also explore the relationship between limited bit precision and the approximation error, providing quantitative answers to the first question we asked above.  These optimal approximate samplers can play a role in several places within probabilistic programming: (1) By replacing discrete samplers for frequently-used probabilistic primitives, each simulation step can be sped up, especially when only limited precision is needed or a given approximation error is allowed. (2) Gibbs steps can sometimes be sped up by using more efficient samplers for the each conditional probability. (3) Discrete custom inference proposals can be replaced by faster approximate sampler; here the use of KL divergence (as the approximation) may be especially relevant.  [This work appears in POPL 2020, January 19-25, 2020: Optimal Approximate Sampling From Discrete Probability Distributions. Feras Saad, Cameron Freer, Martin Rinard, Vikash Mansinghka. Proceedings of the ACM on Programming Languages, December 2019, Article No. 36.]",,,Feras Saad (Massachusetts Institute of Technology); Cameron Freer (Massachusetts Institute of Technology)*; Martin Rinard (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Saad, Feras; Freer, Cameron*; Rinard, Martin; Mansinghka, Vikash",fsaad@mit.edu; freer@mit.edu*; rinard@csail.mit.edu; vkm@mit.edu,,Thu,1,,,Statistics and Data Analysis,"Languages, Tools, and Systems; The Practice of Probabilistic Programming",Syndicated Submission,Accept,
15,1/9/2020 1:29:16 PM -08:00,1/10/2020 7:36:12 PM -08:00,Near-optimal exact sampling for probabilistic programming,"Probabilistic programs often make use of a relatively small amount of input randomness, but even in these cases it can be difficult to analyze how limiting the randomness of subroutines affects the resulting distributions. As such, it is important to study how to efficiently produce exact samples from a given discrete probability distribution, and this fundamental problem has implications even for probabilistic programs involving continuous random variables, noisy data, or approximate inference methods.  This paper introduces a new algorithm for efficiently sampling from a discrete probability distribution using a source of independent and unbiased random coin flips. The size of the sampler is guaranteed to be linear in the number of bits needed to encode the target distribution and the sampler consumes (in expectation) at most 6.5 bits of entropy more than the information-theoretically minimal rate, independently of the values or size of the target distribution.  Empirical evaluations of our implementation of this method establish that it is 2x-10x faster than multiple baseline algorithms for exact sampling, including the widely-used alias and interval samplers.  It also uses up to 10000x less space than the information-theoretically optimal sampler, at the expense of a less than 1.5x runtime overhead.  Because the sampler is exact, it is safe to simply replace the probabilistic primitives in existing libraries with this algorithm, providing potentially substantial speed-up for both simulation and inference. Further, by writing custom inference procedures (e.g., for Gibbs steps) using this sampler, there are further opportunities for efficient inference in probabilistic programs.  [This work appears in AISTATS 2020: The Fast Loaded Dice Roller: Near-Optimal Exact Sampling from Discrete Probability Distributions. Feras Saad, Cameron Freer, Martin Rinard, Vikash Mansinghka. AISTATS 2020.]",,,Feras Saad (Massachusetts Institute of Technology); Cameron Freer (Massachusetts Institute of Technology)*; Martin Rinard (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Saad, Feras; Freer, Cameron*; Rinard, Martin; Mansinghka, Vikash",fsaad@mit.edu; freer@mit.edu*; rinard@csail.mit.edu; vkm@mit.edu,,Thu,1,,,Statistics and Data Analysis,"Languages, Tools, and Systems; The Practice of Probabilistic Programming",Syndicated Submission,Accept,
16,1/9/2020 1:50:05 PM -08:00,1/9/2020 2:27:06 PM -08:00,ArviZ: backend agnostic exploratory analysis of Bayesian models in Python,"In recent years, many libraries have been built to specify probabilistic models as executable code. ArviZ proposes the use of a common data structure called InferenceData to ease and unify common types of analyses of these models. To this end, ArviZ converts results from several libraries to InferenceData objects. It also provides functions to flexibly plot the results using different libraries, calculate relevant diagnostics, or perform model checking. We therefore hope ArviZ will become a key tool in Bayesian analysis by providing users with computational tools to analyze and explore their results, and to compare across different probabilistic programming libraries (PPL). By unifying the common pre and post modelling toolset the workflow is simpler for both PPL designers and Bayesian practitioners. For practitioners it simplifies the steps after inference. For PPL designers it allows them to focus on building their modelling langauge and inference, and deferring pre and post inference tasks to ArviZ.",,,"Oriol Abril Pla (ArviZ)*, Alex Andorra (ArviZ), Agustina Arroyuelo (Instituto de Matemática Aplicada San Luis, UNSL-CONICET), Seth Axen (University of California, San Francisco), Colin Carroll (Freebird, Inc), Piyush Gautam (NIT Hamirpur), Ari Hartikainen (Aalto University), Ravin Kumar (ArviZ), Osvaldo A Martin (Instituto de Matemática Aplicada San Luis, UNSL-CONICET), Mitzi Morris (Institute for Social and Economic Research and Policy, Columbia University), Aki Vehtari (Department of Computer Science, Aalto University).","Abril Pla, Oriol*; Carroll, Colin; Hartikainen, Ari ; Kumar, Ravin ; Martin, Osvaldo",oriol.abril.pla@gmail.com*; colcarroll@gmail.com; ari.hartikainen@aalto.fi; RavinKumar@gmail.com; aloctavodia@gmail.com,,Fri,1,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
17,1/9/2020 4:28:40 PM -08:00,1/10/2020 1:04:02 PM -08:00,Efficient inference with discrete parameters in Stan,"Probabilistic programming languages are often forced to restrict the range of supported models in order to make Bayesian inference feasible. For example, Stan uses an efficient Hamiltonian Monte Carlo (HMC)  algorithm, but supports only models where the posterior is a differentiable function. This means that models with discrete (latent) parameters cannot be used in Stan, which has been one of the main criticisms of the language. A workaround exists, where a Stan user works with discrete parameters implicitly, marginalising them out of the target density, and carefully rearranging the program to draw them using the provided (pseudo-)random number generators. This task can be tedious and error-prone even for experienced users.  We introduce an automatic transformation procedure that takes a Stan program with discrete parameters and transforms it to a Stan program without discrete parameters, where variable elimination (VE) is used to marginalise them out. Running the transformed program in the context of HMC has the effect of running a hybrid VE-HMC inference algorithm on the original program, where the discrete parameters are drawn using forward-filter backward-sampling (FFBS), and the continuous parameters are drawn using HMC. ",,,Maria I Gorinova (University of Edinburgh)*; Andy Gordon (Microsoft Research); Charles Sutton (Google); Matthijs Vakar (University of Oxford),"Gorinova, Maria I*; Gordon, Andy; Sutton, Charles; Vakar, Matthijs",m.gorinova@ed.ac.uk*; adg@microsoft.com; csutton@inf.ed.ac.uk; matthijsvakar@gmail.com,Yes,Fri,1,,Fri,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Talk,
18,1/9/2020 4:56:05 PM -08:00,1/10/2020 11:49:22 PM -08:00,Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous Variables,"Hamiltonian Monte Carlo (HMC) has emerged as a powerful Markov Chain Monte Carlo (MCMC) method to sample from complex continuous distributions. However, a fundamental limitation of HMC is that it can't be applied to distributions with mixed discrete and continuous variables. In this paper, we propose mixed HMC as a general framework to address this limitation, establish its theoretical properties, and present its efficient implementation with Laplace momentum. The superior performance of mixed HMC over existing methods is demonstrated with numerical experiments on Gaussian mixture models (GMMs). ",,,Guangyao Zhou (Vicarious AI)*,"Zhou, Guangyao*",stannis@vicarious.com*,Yes,Thu,1,,Sat,Statistics and Data Analysis,"Languages, Tools, and Systems",Extended Abstract,Talk,
19,1/9/2020 7:48:43 PM -08:00,1/9/2020 7:48:43 PM -08:00,Amortized Population Gibbs Samplers with Neural Sufficient Statistics,"We develop amortized population Gibbs (APG) samplers, a new class of autoencoding variational methods for deep probabilistic models. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. Each conditional update is a neural proposal, which we train by minimizing the inclusive KL divergence relative to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics, resulting in quasi-conjugate variational approximations. Experiments demonstrate that learned proposals converge to the known analytical conditional posterior in conjugate models, and that APG samplers can learn inference networks for highly-structured deep generative models when the conditional posteriors are intractable. ",,,Hao Wu (Northeastern University)*; Heiko Zimmermann (Northeastern University); Eli Sennesh (Northeastern University); Tuan Anh Le (MIT); Jan-Willem van de Meent (Northeastern University),"Wu, Hao*; Zimmermann, Heiko; Sennesh, Eli; Le, Tuan Anh; van de Meent, Jan-Willem",haowu@ccis.neu.edu*; hzimmermann@ccs.neu.edu; esennesh@ccs.neu.edu; tuananh@mit.edu; jwvdm@ccs.neu.edu,,Thu,1,,,Statistics and Data Analysis,Artificial and Natural Intelligence; The Practice of Probabilistic Programming,Extended Abstract,Accept,
21,1/9/2020 8:41:04 PM -08:00,1/9/2020 8:41:37 PM -08:00,PyTorch-Struct: A library for efficient deep structured prediction,"We present PyTorch-Struct, an open-source library for structured prediction through automatic differentiation. Unlike standard neural network operations which can be implemented directly in high-level control scripts, many probabilistic structured prediction algorithms require efficient dynamic programming implementations for computing marginals, entropy, or sampling. PyTorch-Struct provides custom implementations of core distributions in structured prediction as conditional random fields utilizing a standard distributional API of languages like Pyro. The full code is available at https://github.com/harvardnlp/pytorch-struct.",,,Alexander Rush (Harvard)*,"Rush, Alexander*",srush@seas.harvard.edu*,,Thu,1,,,"Languages, Tools, and Systems",Artificial and Natural Intelligence,Extended Abstract,Accept,
22,1/10/2020 1:37:22 AM -08:00,1/10/2020 1:39:05 AM -08:00,Probabilistic programming for birth-death models of evolution using an alive particle filter with delayed sampling,"We consider probabilistic programming for birth-death models of evolution and introduce a new widely-applicable inference method that combines an extension of the alive particle filter (APF) with automatic Rao-Blackwellization via delayed sampling. Birth-death models of evolution are an important family of phylogenetic models of the diversification processes that lead to evolutionary trees. Probabilistic programming languages (PPLs) give phylogeneticists a new and exciting tool: their models can be implemented as probabilistic programs with just a basic knowledge of programming. The general inference methods in PPLs reduce the need for external experts, allow quick prototyping and testing, and accelerate the development and deployment of new models. We show how these birth-death models can be implemented as simple programs in existing PPLs, and demonstrate the usefulness of the proposed inference method for such models. For the popular BiSSE model the method yields an increase of the effective sample size and the conditional acceptance rate by a factor of 30 in comparison with a standard bootstrap particle filter. Although concentrating on phylogenetics, the extended APF is a general inference method that shows its strength in situations where particles are often assigned zero weight. In the case when the weights are always positive, the extra cost of using the APF rather than the bootstrap particle filter is negligible, making our method a suitable drop-in replacement for the bootstrap particle filter in probabilistic programming inference. ",,,Jan Kudlicka (Uppsala University)*; Lawrence M Murray (Uber AI); Fredrik Ronquist (Swedish Museum of Natural History); Thomas Schön (Uppsala University),"Kudlicka, Jan*; Murray, Lawrence M; Ronquist, Fredrik; Schön, Thomas",jan.kudlicka@it.uu.se*; lawrence.murray@uber.com; fredrik.ronquist@nrm.se; thomas.schon@it.uu.se,Yes,Fri,1,,Fri,Statistics and Data Analysis,The Practice of Probabilistic Programming,Syndicated Submission,Talk,
23,1/10/2020 2:32:33 AM -08:00,1/10/2020 1:02:10 PM -08:00,"Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support","Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich and complex probabilistic models. However, this expressiveness comes at the cost of substantially complicating the process of drawing inferences from the model. In particular, inference can become challenging when the support of the model varies between executions. Though general-purpose inference engines have been designed to operate in such settings, they are typically inefficient, often relying on proposing from the prior to make transitions. To address this, we introduce a new inference framework: Divide, Conquer, and Combine (DCC). DCC divides the program into separate straight-line sub-programs, each of which has a fixed support allowing more powerful inference algorithms to be run locally, before recombining their outputs in a principled fashion. We show how DCC can be implemented as an automated and general-purpose PPS inference engine, and empirically confirm that it can provide substantial performance improvements over previous approaches.",,,Yuan Zhou (University of Oxford)*; Hongseok Yang (KAIST); Yee Whye Teh (University of Oxford); Tom Rainforth (University of Oxford),"Zhou, Yuan*; Yang, Hongseok; Teh, Yee Whye; Rainforth, Tom",yuan.zhou@cs.ox.ac.uk*; hongseok00@gmail.com; y.w.teh@stats.ox.ac.uk; rainforth@stats.ox.ac.uk,Yes,Fri,1,,Fri,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Talk,
24,1/10/2020 6:04:26 AM -08:00,1/10/2020 6:04:26 AM -08:00,Higher-Order Probabilistic Programming and Name Generation,"We illustrate the close connections between name generation and probabilistic programming. By interpreting name generation as random sampling, we obtain a new denotational semantics for name generation. Using a novel analysis of distributions on function spaces, we can show that this semantics is actually very fine-grained.",,,Marcin Sabok (McGill University); Sam Staton (University of Oxford); Dario M Stein (University of Oxford)*; Michael Wolman (McGill University),Marcin Sabok; Sam Staton; Dario M Stein*; Michael Wolman,dario.stein@cs.ox.ac.uk*; sam.staton@cs.ox.ac.uk; michael.wolman@mail.mcgill.ca,,Fri,1,,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
25,1/10/2020 7:17:00 AM -08:00,1/11/2020 3:53:36 AM -08:00,SPLog: Sum-Product Logic,"Deep probabilistic programming languages aim to incorporate deep neural networks into probabilistic programming. In the relational setting, the recent DeepProbLog system aims for similar goals by using deep neural networks as conditional density estimators. However, the focus on conditional models limits the types of probabilistic inferences possible. In this extended abstract, we propose to augment ProbLog programs with sum-product networks, tractable estimators for joint probability distributions. The resulting system, which we call SPLog for Sum-Product Logic, allows for programs that include flexible queries to probabilistic models.",,,Arseny Skryagin (TU Darmstadt)*; Karl Stelnez (TU Darmstadt); Alejandro Molina (TU Darmstadt); Fabrizio G Ventola (TU Darmstadt); Kristian Kersting (TU Darmstadt),"Skryagin, Arseny*; Stelnez, Karl; Molina, Alejandro; Ventola, Fabrizio G; Kersting, Kristian",arseny.skryagin@cs.tu-darmstadt.de*; stelzner@cs.tu-darmstadt; molina@cs.tu-darmstadt.de; ventola@cs.tu-darmstadt.de; kersting@cs.tu-darmstadt.de,,Fri,1,,,"Languages, Tools, and Systems",Artificial and Natural Intelligence,Extended Abstract,Accept,
26,1/10/2020 8:48:56 AM -08:00,1/10/2020 3:44:07 PM -08:00,Bayesian causal inference via probabilistic program synthesis,"The Bayesian approach to learning causal models combines a prior distribution over possible models with likelihoods accounting for both observational and experimental data. In a sufficiently expressing probabilistic programming language, such priors can be represented as programs that generate source code in a more restricted DSL of causal generative programs. Experimental data can then be modeled as arising from edited versions of those programs. Since experimental designs can be encoded as arbitrary program transformations, it is straightforward to incorporate experimental data not well-modeled by traditional atomic (""do"") interventions. This abstract describes a prototype approach in Gen, using its inference library to estimate both structure and parameters of a causal model given observational and experimental data.",,,"Sam A Witty (University of Massachusetts, Amherst)*; Alexander K. Lew (MIT); David Jensen (University of Massachusetts Amherst); Vikash Mansinghka (Massachusetts Institute of Technology)","Witty, Sam A*; Lew, Alexander K.; Jensen, David; Mansinghka, Vikash",switty@cs.umass.edu*; alexlew@mit.edu; jensen@cs.umass.edu; vkm@mit.edu,,Fri,1,,,Statistics and Data Analysis,Artificial and Natural Intelligence,Extended Abstract,Accept,
27,1/10/2020 9:18:13 AM -08:00,1/10/2020 11:47:14 PM -08:00,tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern Hardware,"Markov chain Monte Carlo (MCMC) is widely regarded as one of the most important algorithms of the 20th century. Its guarantees of asymptotic convergence, stability, and estimator-variance bounds using only unnormalized probability functions make it indispensable to probabilistic programming. In this paper, we introduce the TensorFlow Probability MCMC toolkit, and discuss some of the considerations that motivated its design.",,,Junpeng Lao (Google); Christopher Suter (Google); Ian Langmore (Google); Cyril Chimisov (Google); Ashish Saxena (Google); Pavel Sountsov (Google); Dave Moore (Google); Rif A. Saurous (); Matthew D Hoffman (Google); Joshua V Dillon (Google)*,"Lao, Junpeng; Suter, Christopher; Langmore, Ian; Chimisov, Cyril; Saxena, Ashish; Sountsov, Pavel; Moore, Dave; A. Saurous, Rif; Hoffman, Matthew D; Dillon, Joshua V*",junpenglao@google.com; cgs@google.com; langmore@google.com; cyrilchimisov@google.com; saxeas@google.com; siege@google.com; davmre@google.com; rif@google.com; mhoffman@google.com; jvdillon@google.com*,,Thu,1,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
28,1/10/2020 9:51:52 AM -08:00,1/10/2020 9:51:52 AM -08:00,Modular Exact Inference for Discrete Probabilistic Programs,"Scaling probabilistic inference remains a hurdle for applying probabilistic programming languages (PPLs) to practical problems of interest. One of the most successful strategies for scaling program analyses to large programs is modularity: breaking the analysis up along natural program boundaries (such as procedures), analyzing each sub-unit in isolation, and then combining the results to perform the analysis on the entire program. Despite its success in many domains, current approaches to probabilistic inference are not modular, but rather analyze the whole program as a unit.  In this work, we address this limitation of existing inference algorithms by introducing Dice, a PPL that supports modular exact inference for discrete probabilistic programs. Our key technical contribution is the introduction of a new compositional compilation target that supports efficient exact inference. We show how to separately compile Dice functions to this target and demonstrate the performance benefits over prior approaches on examples from network verification, text analysis, graph analysis, and probabilistic graphical models. ",,,"Steven J Holtzen (University of California, Los Angeles)*; Guy Van den Broeck (UCLA); Todd Millstein (UCLA)","Holtzen, Steven J*; Van den Broeck, Guy; Millstein, Todd",sholtzen@cs.ucla.edu*; guyvdb@cs.ucla.edu; todd@cs.ucla.edu,Yes,Thu,1,,Sat,"Languages, Tools, and Systems",,Extended Abstract,Talk,
29,1/10/2020 10:17:40 AM -08:00,1/11/2020 2:53:29 AM -08:00,Compilation of Universal Probabilistic Programs to GPGPUs,"This work-in-progress paper describes an effort of creating a highly efficient compilation and runtime environment for probabilistic programs. In particular, the work consists of three parts: (i) the formal definition of the core of a universal probabilistic programming language (PPL) from which domain-specific PPLs can be derived, (ii) static analysis and efficient compilation of the core language down to General Purpose Graphical Processing Unit (GPGPU) code, and (iii) an efficient probabilistic inference and runtime engine that executes the program on a GPGPU.  The probabilistic core language (called PPLCore) is developed as part of Miking—a framework for constructing domain-specific languages and compilers. The current GPU engine is based on Sequential Monte Carlo (SMC) inference and systematic parallel resampling. The overall toolchain is a work-in-progress effort, where the GPU engine and the formalization of the core language are rather mature, whereas the compilation process is still at a very early stage. ",,,Daniel Lundén (KTH Royal Institute of Technology)*; Joey Öhman (KTH Royal Institute of Technology); David Broman (),"Lundén, Daniel*; Öhman, Joey; Broman, David",dlunde@kth.se*; joeyoh@kth.se; dbro@kth.se,,Fri,1,,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
30,1/10/2020 10:20:29 AM -08:00,1/10/2020 9:06:10 PM -08:00,DynamicPPL: Stan-like speed for dynamic probabilistic models,"We present the preliminary high-level design and features of DynamicPPL.jl, a modular library providing a lightning-fast infrastructure for probabilistic programming. Besides a computational performance that is often close to or better than Stan, DynamicPPL provides an intuitive DSL that allows the rapid development of complex dynamic probabilistic programs. Being entirely written in Julia, a high-level dynamic programming language for numerical computing, DynamicPPL inherits a rich set of features available through the Julia ecosystem. Since DynamicPPL is a modular, stand-alone library, any probabilistic programming system written in Julia, such as Turing.jl, can use DynamicPPL to specify models and trace their model parameters. The main features of DynamicPPL are: 1) a meta-programming based DSL for specifying dynamic models using an intuitive tilde-based notation; 2) a tracing data-structure for tracking RVs in dynamic probabilistic models; 3) a rich contextual dispatch system allowing tailored behaviour during model execution; and 4) a user-friendly syntax for probabilistic queries. Finally, we show in a variety of experiments that DynamicPPL, in combination with Turing.jl, achieves computational performance that is often close to or better than Stan.",,,"Mohamed Tarek (University of New South Wales, Canberra, Australia); Kai Xu (University of Edinburgh); Martin Trapp (Graz University of Technology); Hong Ge (University of Cambridge)*; Zoubin Ghahramani (University of Cambridge)","Tarek, Mohamed; Xu, Kai; Trapp, Martin; Ge, Hong*; Ghahramani, Zoubin",m.mohamed@student.adfa.edu.au; kai.xu@ed.ac.uk; martin.trapp@tugraz.at; hg344@cam.ac.uk*; zoubin@eng.cam.ac.uk,,Fri,1,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
32,1/10/2020 11:58:12 AM -08:00,1/10/2020 11:58:12 AM -08:00,Compiling Stan to Generative Probabilistic Languages,"Stan is a popular declarative probabilistic programming language with a high-level syntax for expressing graphical models and beyond. Stan differs by nature from generative probabilistic programming languages like Church, Anglican, or Pyro. In this paper, we present a comprehensive compilation scheme to compile any Stan model to a generative language. We use this result to build a compiler from Stan to Pyro and extend extend Stan with support for explicit variational inference guides and deep probabilistic models. Overall, our paper clarifies the relationship between declarative and generative probabilistic programming languages and is a step towards making deep probabilistic programming easier.",,,Guillaume Baudart (IBM Research)*; Javier Burroni (UMass Amherst); Martin Hirzel (IBM Research); Kiran Kate (IBM Research); Louis Mandel (IBM Research); Avraham Shinnar (IBM Research),"Baudart, Guillaume*; Burroni, Javier; Hirzel, Martin; Kate, Kiran; Mandel, Louis; Shinnar, Avraham",guillaume.baudart@ibm.com*; javier.burroni@gmail.com; hirzel@us.ibm.com; kakate@us.ibm.com; lmandel@us.ibm.com; shinnar@us.ibm.com,,Thu,1,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
33,1/10/2020 12:09:39 PM -08:00,1/10/2020 12:09:39 PM -08:00,Nested Reasoning About Autonomous Agents,"As autonomous agents become more ubiquitous, they will eventually have to reason about the plans of other agents,  which is known as theory of mind reasoning. We use probabilistic programs to model a high-uncertainty variant of pursuit-evasion games in which an agent must make inferences about the other agents' plans to craft counter-plans. Our probabilistic programs incorporate a variety of complex primitives such as field-of-view calculations and path planners, which enable us to model quasi-realistic scenarios in a computationally tractable manner. We perform extensive experimental evaluations which establish a variety of rational behaviors and quantify how allocating computation across levels of nesting affects the variance of our estimators.",,,Iris R Seaman (Northeastern University)*; Jan-Willem van de Meent (Northeastern University); David Wingate (Brigham Young University),"Seaman, Iris R*; van de Meent, Jan-Willem; Wingate, David",seaman.i@husky.neu.edu*; j.vandemeent@northeastern.edu; wingated@cs.byu.edu,Yes,Thu,2,,Thu,Artificial and Natural Intelligence,Statistics and Data Analysis; The Practice of Probabilistic Programming,Extended Abstract,Talk,
34,1/10/2020 12:25:55 PM -08:00,1/10/2020 12:25:55 PM -08:00,The Design of Scruff: A Framework for AI Based on Probabilistic Programming,"Our goal is to use probabilistic programming as the foundation for building full-fledged AI systems that sense, monitor, reason, learn, and act in the world. Scruff is an AI framework based on predictive coding designed to achieve this goal. In a previous paper, we presented an initial proposal for the design of Scruff, focused on asynchronous belief propagation as a natural fit for predictive coding. Since then, the design of Scruff has proceeded considerably and is now a highly general AI framework capable of supporting many kinds of models and algorithms. In this paper, we introduce the main design principles of Scruff. We first present a use case using a hierarchical predictive knowledge representation. We then present the ingredients of an AI system built using Scruff and describe how they interact.",,,Avi Pfeffer (Charles River Analytics)*; Jarred Barber (Charles River Analytics); McCoy Becker (Charles River Analytics); Joe Campolongo (Charles River Analytics); Joe Gorman (Charles River Analytics); Michael R Harradon (Charles River Analytics); Kenneth Lu (Charles River Analytics); Steve Wacks (Charles River Analytics),"Pfeffer, Avi*; Barber, Jarred; Becker, McCoy; Campolongo, Joe; Gorman, Joe; Harradon, Michael R; Lu, Kenneth; Wacks, Steve",apfeffer@cra.com*; jbarber@cra.com; mbecker@cra.com; jcampolongo@cra.com; jgorman@cra.com; mharradon@cra.com; klu@cra.com; swacks@cra.com,,Fri,1,Avi Pfeffer,,Artificial and Natural Intelligence,"Languages, Tools, and Systems",Extended Abstract,Accept,
35,1/10/2020 12:55:24 PM -08:00,1/10/2020 12:55:24 PM -08:00,Automated Posterior Interval Evaluation for Inference in Probabilistic Programming,"In probabilistic inference, credible intervals constructed from posterior samples provide ranges of likely values for continuous parameters of interest. Intuitively, an inference procedure is optimal if it produces the most precise posterior intervals that cover the true parameter value with the expected frequency in repeated experiments. We present theories and methods for automating posterior interval evaluation of inference performance in probabilistic programming using two metrics: (1) truth coverage, and (2) ratio of the empirical over the ideal interval widths. In an intuitive demonstration on simple Bayesian linear regression, we show how the metrics provide effective comparisons between different inference procedures, and capture the effects of collinearity and model misspecification. Overall, we claim such automated interval evaluation can accelerate the robust design and comparison of probabilistic inference programs, by directly diagnosing how accurately and precisely they can estimate the truth.",,,Edward K Kao (MIT-LL)*; Michael Yee (MIT-LL),"Kao, Edward K*; Yee, Michael",edward.kao@ll.mit.edu*; myee@ll.mit.edu,,Thu,1,,,Inference Performance Evaluation,Statistics and Data Analysis,Extended Abstract,Accept,
36,1/10/2020 1:04:40 PM -08:00,1/10/2020 1:04:40 PM -08:00,Lazy Structured Factored Inference: A Highly General Algorithm for Probabilistic Program Inference,"Probabilistic programming languages (PPLs) provide expressive representations and general algorithms that are intended to work on models in the language. Unfortunately, the representational capacity of languages generally outstrips the capability of algorithms to answer queries.  In particular, models that do not terminate with probability one and models with mixed discrete and continuous variables commonly cause problems. In this paper, we present lazy structured factored inference (LSFI) to address both these issues. LSFI lazily expands a data structure called the solution graph, which represents a prefix of the inference problem. At each iteration, it refines the solution graph, either by increasing the recursion depth or by refining the partitioning of continuous variables, and then solves the soliution graph using dynamic programming to bound the answer to the query. We show that these bounds do not get worse over time and, in the case of a program that terminates with probability one, converge to the correct answer to the query. For programs that diverge, we conjecture that the bounds converge to the interval of possible answers to the query.",,,Avi Pfeffer (Charles River Analytics)*; Brian Ruttenberg (formerly of Charles River Analytics); William Kretschmer (University of Texas),"Pfeffer, Avi*; Ruttenberg, Brian; Kretschmer, William",apfeffer@cra.com*; ruttenberg@gmail.com; kretsch@cs.utexas.edu,,Fri,1,Avi Pfeffer,,"Languages, Tools, and Systems",Artificial and Natural Intelligence,Extended Abstract,Accept,
37,1/10/2020 1:32:09 PM -08:00,1/11/2020 12:08:10 AM -08:00,Structured Conditional Continuous Normalizing Flows,"We introduce structured, sparse conditional continuous normalizing flows in which both the number of parameters and of adaptive integration steps  are reduced both at training and inference time relative to existing non-sparse  alternatives.  We achieve this without decreasing density estimation performance in comparison to non-sparse flows. Such models can be constructed  in situations where the dependency structure of the conditional density being estimated can be derived from side information. We identify common situations where such side information is available and develop formal programming language tools to automatically derive this sparsity structure.",,,Christian Weilbach (University of British Columbia)*; Boyan Beronov (University of British Columbia); William S G Harvey (University of British Columbia); Frank Wood (University of British Columbia),"Weilbach, Christian*; Beronov, Boyan; Harvey, William S G; Wood, Frank",ch_weil@topiq.es*; beronov@cs.ubc.ca; williamharvey08@gmail.com; fwood@cs.ubc.ca,,Thu,1,,,Statistics and Data Analysis,"Languages, Tools, and Systems",Syndicated Submission,Accept,
38,1/10/2020 2:22:06 PM -08:00,1/10/2020 2:29:43 PM -08:00,Generative Hap: probabilistic DSL for agent inference,"Inferring latent agent state by observing agent action is a longstanding problem in the field of machine learning. Traditional approaches which rely on techniques from plan and activity recognition are brittle to variations in agent behavior caused by high-level cognitive activity (like multitasking or reactive adaptation). Approaches which rely on probabilistic programming are more promising - a probabilistic program can represent in distribution an infinite set of possible agent behaviors and goals. When an agent is expressed in this way, the problem becomes one of Bayesian inference. However, a complex probabilistic description of an agent expressed in a naive way is likely to produce an inference problem which is intractable. To combat this, we present a probabilistic version of the Hap agent specification language called Generative Hap which is flexible enough to express agents with complex cognitive activity (like concurrent multitasking and goal switching) but has a probabilistic runtime which reduces to a dynamic Bayesian network. In addition, the language semantics are restricted in such a way as to enable the acceleration of inference algorithms over the DBN. ",,,McCoy R Becker (Charles River Analytics)*; Bryan Loyall (Charles River Analytics); Avi Pfeffer (Charles River Analytics); Kenneth Lu (Charles River Analytics); Peter Galvin (Charles River Analytics),"Becker, McCoy R*; Loyall, Bryan; Pfeffer, Avi; Lu, Kenneth; Galvin, Peter",mccoybecker@gmail.com*; bloyall@cra.com; apfeffer@cra.com; klu@cra.com; pgalvin@cra.com,,,,,,Artificial and Natural Intelligence,"Languages, Tools, and Systems",Extended Abstract,Withdrawn,
39,1/10/2020 2:49:11 PM -08:00,1/10/2020 2:53:23 PM -08:00,A Multilevel Bayesian Model for Precision Oncology,"The challenge of personalized medicine is to predict the causal effect of a treatment on patient, given a number of clinically relevant patient features. This task requires a flexible model that can integrate heterogeneous data, be easily interpreted by domain experts, and provide a meaningful quantification of the uncertainty in the prediction. In this submission we describe such a tool in the form of a multilevel Bayesian model for precision oncology, implemented in the probabilistic programming language, Stan, and we discuss the application of the model to brain cancer patient treatment outcomes.",,,Asher Wasserman (xCures)*; Jeff Shrager (xCures); Mark Shapiro (xCures); Al Musella (The Musella Foundation For Brain Tumor Research & Information),"Wasserman, Asher*; Shrager, Jeff; Shapiro, Mark; Musella, Al",asher.d.wasserman@gmail.com*; jshrager@xcures.com; mshapiro@xcures.com; musella@virtualtrials.com,,Thu,1,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
40,1/10/2020 3:26:51 PM -08:00,1/10/2020 3:36:50 PM -08:00, Deep Probabilistic Surrogate Networks for Universal Simulator Approximation,"We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of existing stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure of the reference simulators. The particular way we achieve this allows us to replace the reference simulator with the surrogate when undertaking amortized inference in the probabilistic programming sense. The fidelity and speed of our surrogates allow for not only faster ""forward"" stochastic simulation but also for accurate and substantially faster inference. We support these claims via experiments that involve a commercial composite-materials curing simulator. Employing our surrogate modeling technique makes inference an order of magnitude faster, opening up the possibility of doing simulator-based, non-invasive, just-in-time parts quality testing; in this case inferring safety-critical latent internal temperature profiles of composite materials undergoing curing from surface temperature profile measurements.",,,Andreas Munk (University of British Columbia)*; Adam Scibior (University of British Columbia); Atilim Gunes Baydin (University of Oxford); Andrew Stewart (Convergent Manufacturing Technologies Inc.); Goran Fernlund (Convergent Manufacturing Technologies Inc.); Anoush Poursartip (University of British Columbia); Frank Wood (University of British Columbia),"Munk, Andreas*; Scibior, Adam; Baydin, Atilim Gunes; Stewart, Andrew; Fernlund, Goran; Poursartip, Anoush; Wood, Frank",amunk@cs.ubc.ca*; ascibior@cs.ubc.ca; gunes@robots.ox.ac.uk; andrew.stewart@convergent.ca; goran.fernlund@convergent.ca; anoush.poursartip@ubc.ca; fwood@cs.ubc.ca,,Thu,1,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
41,1/10/2020 3:37:17 PM -08:00,1/10/2020 3:37:17 PM -08:00,Probabilistic analysis of experimental DEER spectroscopy data for protein structure determination,"Having access to a reliable structure of a protein is crucial to understand the dynamics of proteins, which is required for the development of novel therapeutics. The double electron-electron resonance (DEER) experiment is a spectroscopic method that measures the distances between unpaired electrons in proteins. Obtaining a distance distribution from experimental data involves the solution of an ill-posed inverse problem which is often done with Tikhonov regularization. The resulting distance distribution lacks error bars which makes interpretation difficult. Instead of relying on regularization, the distance distribution can be approximated by one or multiple features of Gaussian shape, all of which are defined by an amplitude, position and width.  Using a Bayesian approach and Markov chain Monte-Carlo (MCMC) sampling we developed a tool box, based on python, Julia, and Stan, to obtain posterior distributions for all parameters that are used to model DEER signals. In this work, we compare our approach to other software tools that use regularization to analyze DEER data and discuss the additional insight gained by using MCMC methods.",,,Stephan Pribitzer (University of Washington)*; Sarah  Sweger (University of Washington); Stefan Stoll (University of Washington),"Pribitzer, Stephan*; Sweger, Sarah ; Stoll, Stefan",stephapr@uw.edu*; sweger19@uw.edu; stst@uw.edu,,Thu,1,,,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",Extended Abstract,Accept,
42,1/10/2020 4:52:00 PM -08:00,1/10/2020 4:52:00 PM -08:00,Joint Distributions for TensorFlow Probability,"A central tenet of probabilistic programming is that a model is specified exactly once in a canonical representation which is usable by inference algorithms. We describe JointDistributions, a family of declarative representations of directed graphical models in TensorFlow Probability.",,,Dan Piponi (Google)*; Dave Moore (Google); Joshua V Dillon (Google),"Piponi, Dan*; Moore, Dave; Dillon, Joshua V",dpiponi@google.com*; davmre@google.com; jvdillon@google.com,,Thu,1,,,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,Extended Abstract,Accept,
43,1/10/2020 4:52:32 PM -08:00,1/10/2020 4:52:32 PM -08:00,Analysis of Distributed Training of Bayesian Neural Networks at Scale,"Deep neural networks (DNN) are accelerating the pace of research in many scientific domains such as medical science, biological and materials science, weather prediction, computer vision and chemistry. However, the major limitation of these feed forward networks is the lack of robust uncertainty quantification for the predictions. Bayesian neural networks (BNN) can address these limitations with some additional computational overhead. Understanding how these networks perform and any potential bottlenecks are crucial in obtaining results at scale. In this work we present a performance and scalability comparison of the VGG-16, Resnet convolutional network, with and without Bayesian layers, using the distributed training framework Horovod on the Cray-X40 system, Theta, at Argonne Leadership Computing Facility. The study will provide guidelines for training a Bayesian neural network at scale and will aid in understanding the limitations and benefits over a conventional neural network. ",,,Himanshu Sharma (Argonne National Lab)*; Elise Jennings (Argonne National Lab),"Sharma, Himanshu*; Jennings, Elise",himanshu90sharma@gmail.com*; ejennings@anl.gov,,Thu,1,,,"Languages, Tools, and Systems",Artificial and Natural Intelligence; Statistics and Data Analysis; The Practice of Probabilistic Programming,Extended Abstract,Accept,
44,1/10/2020 4:58:13 PM -08:00,1/10/2020 4:58:13 PM -08:00,PClean: Probabilistic Scripts for Automating Common-Sense Data Cleaning,"A key design challenge in implementing data-cleaning systems is deciding how users should encode domain knowledge relevant to the cleaning task; an ideal system would empower users to ﬂexibly and concisely communicate all sorts of relevant information, then use it — in addition to any patterns learned from the data table itself — to improve accuracy during cleaning. This makes data cleaning an ideal setting in which to study the interplay between machine learning and knowledge representation. Inspired by a recently introduced framework for viewing data cleaning as probabilistic inference, we present PClean, which detects errors, proposes corrections, and imputes missing values in unclean tabular datasets guided by short, declarative, user-written scripts in a domain-speciﬁc probabilistic programming language. We show that these scripts can encode various kinds of knowledge useful for data cleaning, and describe a hybrid sequential Monte Carlo / EM algorithm for learning patterns from the data, detecting and correcting errors, and imputing missing values guided by these probabilistic scripts. We report preliminary results of two experiments, which suggest that PClean’s scripting language can encode assumptions that recover the behavior of existing systems on some datasets. And through an extended example, we show how PClean enables common-sense reasoning about data without any training on labeled examples.",,,Alexander K. Lew (MIT)*; Monica N Agrawal (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Lew, Alexander K.*; Agrawal, Monica N; Mansinghka, Vikash",alexlew@mit.edu*; magrawal@mit.edu; vkm@mit.edu,,Thu,1,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Syndicated Submission,Accept,
45,1/10/2020 5:32:58 PM -08:00,1/10/2020 5:32:58 PM -08:00,A Monad for Point Processes,"A point process on a set is a random bag of elements of that set. In this extended abstract we explore programming with point processes in a monadic interface. This work is based on a monad we mathematically define, which we then implement in Haskell in order to run simulations with. We use this monad to construct various examples of point processes and present an application to writing queries on. probabilistic databases.",,,Swaraj Dash (University of Oxford)*; Sam Staton (University of Oxford),"Dash, Swaraj*; Staton, Sam",dashswaraj@gmail.com*; sam.staton@cs.ox.ac.uk,,Fri,1,,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
46,1/10/2020 5:51:48 PM -08:00,1/11/2020 12:29:20 AM -08:00,"AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms","Stan's Hamilton Monte Carlo (HMC) has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems through carefully crafted adaption schemes to the celebrated No-U-Turn sampler (NUTS) algorithm.  It is challenging to implement these adaption schemes robustly in practice, hindering wider adoption amongst practitioners who are not directly working with the Stan modelling language. AdvancedHMC.jl (AHMC) contributes a modular, well-tested, standalone implementation of NUTS that recovers and extends Stan's NUTS algorithm. AHMC is written in Julia, a modern high-level language for scientific computing, benefiting from optional hardware acceleration and interoperability with a wealth of existing software written in both Julia and other languages, such as Python. Efficacy is demonstrated empirically by comparison with Stan through a third-party Markov chain Monte Carlo benchmarking suite.",,,"Kai Xu (University of Edinburgh)*; Mohamed Tarek (University of New South Wales, Canberra, Australia); Martin Trapp (Graz University of Technology); Hong Ge (University of Cambridge); Zoubin Ghahramani (University of Cambridge); William Tebbutt (University of Cambridge)","Xu, Kai*; Tarek, Mohamed; Trapp, Martin; Ge, Hong; Ghahramani, Zoubin; Tebbutt, William",kai.xu@ed.ac.uk*; m.mohamed@student.adfa.edu.au; martin.trapp@tugraz.at; hg344@cam.ac.uk; zoubin@eng.cam.ac.uk; wct23@cam.ac.uk,,Fri,1,,,Statistics and Data Analysis,"Languages, Tools, and Systems",Syndicated Submission,Accept,
47,1/10/2020 6:00:58 PM -08:00,1/10/2020 6:08:46 PM -08:00,PPLBench: Evaluation Framework For Probabilistic Programming Languages,"We introduce PPLBench, a new benchmark for evaluating Probabilistic Programming Languages (PPLs) on a variety of statistical models. The benchmark includes data generation and evaluation code for a number of models as well as reference implementations in some common PPLs. All of the benchmark code and reference implementations will be made available as open source. We welcome contributions of new models as well as improved reference implementations. The purpose of the benchmark is two-fold. First, we want researchers as well as conference reviewers to be able to evaluate improvements in PPLs in a standardized setting. Second, we want end users to be able to pick the PPL that is most suited for their modeling application. In particular, we are interested in evaluating the accuracy and speed of convergence of the inferred posterior. The benchmark includes extensive evaluation code for MCMC-based inference. Each PPL only needs to provide posterior samples given a model and observation data. The framework automatically computes and plots growth in predictive log-likelihood on held out data in addition to reporting other common MCMC metrics such as effective sample size and r_hat.  ",,,Sourabh Kulkarni; Kinjal Divesh Shah; Nimar Arora; Xiaoyan Wang; Yucen Lily Li; Nazanin Khosravani Tehrani; Michael Tingley; David Noursi; Narjes Torabi; Sepehr Akhavan Masouleh; Eric Lippert; Erik Meijer,,skulkarni@umass.edu,,Thu,1,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
48,1/10/2020 6:05:39 PM -08:00,1/10/2020 6:05:39 PM -08:00,Functional Tensors for Probabilistic Programming,"It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based     in large part on the unifying concept of tensors, we describe a software abstraction--functional tensors--that captures many of the benefits of tensors, while also being able to describe continuous probability distributions.  We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro programming language.  In experiments we show that the resulting framework enables a large variety of inference strategies, including those that mix exact and approximate inference.",,,Fritz Obermeyer (Uber AI Labs); Eli Bingham (Uber AI Labs)*; Martin Jankowiak (Uber AI Labs); Du Phan (UIUC); Jonathan Chen (-),"Obermeyer, Fritz; Bingham, Eli*; Jankowiak, Martin; Phan, Du; Chen, Jonathan",fritzo@uber.com; eli.bingham@uber.com*; jankowiak@uber.com; phandu@postech.ac.kr; jonathanpchen@gmail.com,,Fri,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
49,1/10/2020 6:24:02 PM -08:00,1/10/2020 6:28:18 PM -08:00,Delayed Sampling via Barriers and Funsors,Delayed sampling is an inference technique for automatic Rao-Blackwellization in sequential latent variable models. Funsors are a software abstraction generalizing Tensors and Distributions and supporting seminumerical computation including analytic integration. We demonstrate how to easily implement delayed sampling in an embedded probabilistic programming language using Funsors and effect handlers for sample statements and a new barrier statement.,,,Fritz Obermeyer (Uber AI Labs)*; Eli Bingham (Uber AI Labs),"Obermeyer, Fritz*; Bingham, Eli",fritzo@uber.com*; eli.bingham@uber.com,,Thu,1,,,The Practice of Probabilistic Programming,"Languages, Tools, and Systems",Extended Abstract,Accept,
50,1/10/2020 6:41:26 PM -08:00,1/10/2020 6:41:26 PM -08:00,Eff-Bayes: ProbProg with built-in effect handlers,"We report on work-in-progress implementing modular probabilistic programming libraries in languages with built-in effect handlers. Our architecture follows that of Scibior's monad-bayes, a Haskell library based on monads and type-classes. We first consider a basic implementation comprising of importance sampling and sequential Monte Carlo in Pretnar's Eff language, an OCaml dialect and the prototypical language supporting effect handlers. We then consider a more advanced implementation supporting in addition trace Markov-chain Monte Carlo in Leijen's Koka language, a JavaScript dialect with an advanced type-and-effect system for effect handlers. We report on preliminary testing of the Koka implementation with a simple Kalman filter fitted to a Berkeley Earth climate dataset. ",,,Oliver Goldstein (King's College London); Žiga Lukšič (University of Ljubljana); Matija Pretnar (University of Ljubljana); Daan Leijen (Microsoft Research Redmond); Ohad Kammar (University of Oxford)*; Adam Scibior (University of British Columbia),"Goldstein, Oliver; Lukšič, Žiga; Pretnar, Matija; Leijen, Daan; Kammar, Ohad*; Scibior, Adam",oliver.goldstein@kcl.ac.uk; ziga.luksic@fmf.uni-lj.si; matija.pretnar@fmf.uni-lj.si; Daan@microsoft.com; ohad.kammar@ed.ac.uk*; ascibior@cs.ubc.ca,,Thu,2,Ohad Kammar,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
51,1/10/2020 7:11:15 PM -08:00,1/10/2020 10:13:19 PM -08:00,Bean Machine: A Declarative Probabilistic Programming Language For Efficient Programmable Inference,"A number of imperative Probabilistic Programming Languages (PPLs) have been recently proposed, but the imperative style choice makes it very hard to deduce the dependence structure between the latent variables, which can also change from sample to sample. We propose a new declarative style PPL, Bean Machine, and demonstrate that in this new language the dynamic dependence structure is readily available. Although we are not the first to propose a declarative PPL or to observe the advantages of knowing the dependence structure, but we take the idea further by showing other inference techniques that become feasible or easier in this style. We show that it is very easy for users to program inference by composition (combining different inference techniques for different parts of the model), customization (providing a custom hand-writtten inference method for specific variables), and blocking (specifying blocks of random variables that should be sampled together) in a declarative language. A number of empirical results are provided where we backup these claims modulo the runtime inefficiencies of unvectorized Python. As a fringe benefit, we note that it is very easy to translate statistical models written in mathematical notation into our language. ",,,"Nazanin Tehrani,  Nimar S. Arora, Yucen Lily Li,  Kinjal Divesh Shah, David Noursi,  Michael Tingley, Narjes Torabi, Sepehr Masouleh, Eric Lippert, Erik Meijer
",,,Yes,Thu,2,,Sat,The Practice of Probabilistic Programming,Artificial and Natural Intelligence; Statistics and Data Analysis,Extended Abstract,Talk,Kinjal Shah - talk / David Noursi - poster
52,1/10/2020 7:11:20 PM -08:00,1/11/2020 1:06:11 AM -08:00,Probabilistic Programming by Transformation in JAX,"JAX is a new numeric library characterized by its GPU/TPU-friendliness, a high-level NumPy-like API, and an extensible suite of composable program transformations. In this work, we define explicit program transformations that convert a JAX program (defined as a Python function) that samples from a distribution into various related JAX programs, including one that evaluates the distribution's density. Because we embed in JAX, we can make these transformations highly composable and efficient while retaining the ability to reason about every aspect of the program; for example, we can apply the change-of-variables formula to compute densities of transformed variables. We illustrate the efficacy of the resulting probabilistic programming style by implementing Hamiltonian Monte Carlo on a Bayesian neural network.",,,Sharad Vikram (Google)*; Alexey Radul (Google); Matthew D Hoffman (Google),"Vikram, Sharad*; Radul, Alexey; Hoffman, Matthew D",sharad.vikram@gmail.com*; axch@google.com; mhoffman@google.com,,Thu,2,,,The Practice of Probabilistic Programming,"Languages, Tools, and Systems; Statistics and Data Analysis",Extended Abstract,Accept,
53,1/10/2020 7:25:49 PM -08:00,1/10/2020 7:25:49 PM -08:00,Structured differentiable models of 3D scenes via generative scene graphs,"Generative approaches to 3D scene perception and physical reasoning are increasingly common. There is a widespread need for scene models that can incorporate planar and point-wise contacts between physical objects, and ensure that objects do not inter-penetrate. Generic priors on 6DOF poses --- and bottom-up neural networks that de-render images into scenes --- typically violate these constraints.  This abstract introduces a family of generative models for 3D scenes that respect pairwise physical contact constraints between objects.  The technical innovation is to represent scenes in terms of hybrid symbolic--numerical scene graphs over objects, where the edges correspond to parameterized contact relationships.  Given this representation, 3D poses can be generated via a graph traversal.  We show how to define prior distributions on scene graphs, and how 3D scene understanding can be phrased as posterior inference over the scene graph.  This abstract also shows preliminary evidence that it is possible to infer scene graph parameters from empirical data, ``de-rendering'' images into structured 3D scene representations.  This is implemented using the Gen probabilistic programming language.  Finally, this abstract briefly discusses representation and inference challenges that need to be addressed to scale up the approach to more complex, real-world scenes.",,,Ben Zinberg (Massachusetts Institute of Technology)*; Marco Cusumano-Towner (); Vikash Mansinghka (Massachusetts Institute of Technology),"Zinberg, Ben*; Cusumano-Towner, Marco; Mansinghka, Vikash",bzinberg@alum.mit.edu*; marcoct@mit.edu; vkm@mit.edu,Yes,Thu,2,,Thu,Artificial and Natural Intelligence,"Languages, Tools, and Systems",Syndicated Submission,Talk,
54,1/10/2020 7:49:47 PM -08:00,1/10/2020 7:49:47 PM -08:00,A Bayesian Computation Graph for High-Performance Gradient Evaluation on the JVM,"Rainier is a Scala library for building fixed-structure, continuous-parameter generative models, to be sampled and optimized using gradient-based methods. Its core contribution is a static computation graph targeted at Bayesian model inference in a Java Virtual Machine production environment.  In particular, Rainier is designed to be deployed to large data processing clusters running Spark, Hadoop, or similar JVM-based systems, which are very commonly found in industry. These environments often discourage the use of native (as opposed to JVM) libraries, and do not have access to GPUs. As a result, Rainier must rely on a heavily optimizing compiler, directly targeting the JVM, to achieve high performance.  We will focus on two stages of the compiler: first, a partial evaluation stage that attempts to pre-compute as much as possible of the function represented by the graph, given fixed model inputs and observations (but still allowing parameter values to vary); second, the generation of low-level JVM bytecode designed to be easily compiled to efficient machine code by the JVM's just-in-time compiler.  We will also briefly discuss two novel features of the computation graph that are particularly helpful in the Bayesian context: log-density annotations on parameter nodes, and the use of interval arithmetic for tracking the support of each node of the graph. ",,,Avi Bryant (Gradient Retreat)*,"Bryant, Avi*",avi@avibryant.com*,Yes,Thu,2,,Sat,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Talk,
55,1/10/2020 7:59:24 PM -08:00,1/11/2020 12:01:27 AM -08:00,FunMC: A functional API for building Markov Chains,"Constant-memory algorithms, also loosely called Markov chains, power the vast majority of probabilistic inference and machine learning applications today. A lot of progress has has been made in constructing user- friendly APIs around these algorithms. Such APIs, however, rarely make it easy to research new algorithms of this type. In this work we present FunMC, a minimal Python library for doing methodological research into algorithms based on Markov chains. FunMC is not targeted toward data scientists or others who wish to use MCMC or optimization as a black box, but rather towards researchers implementing new Markovian algorithms from scratch.",,,Pavel Sountsov (Google)*; Alexey Radul (Google),"Sountsov, Pavel*; Radul, Alexey",siege@google.com*; axch@google.com,,Thu,2,,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
56,1/10/2020 8:00:53 PM -08:00,1/11/2020 3:58:57 AM -08:00,Subproblem pseudomarginal reversible jump MCMC in probabilistic programming languages,"Decomposing a probabilistic inference problem into subproblems that can be solved independently or sequentially is a common strategy for Monte Carlo inference. One popular approach uses cycles of MCMC updates to different groups of one or more latent random variables. Several recent Turing-universal probabilistic programming languages designed for user-programmable inference allow the user to specify groups of random variables that should be jointly updated, while keeping others fixed. However, existing MCMC kernels for jointly updating groups of variables in probabilistic programming languages are either inefficient, only work in certain cases, or require carefully crafted proposal distributions. This paper introduces subproblem pseudomarginal reversible jump (SPRJ) MCMC, a broad class of MCMC kernels designed to simultaneously (i) solve general subproblems defined by potentially large groups of random variables, (ii) be easily composed in cycles, (iii) be easy to improve with more computational effort, and (v) seamlessly interoperate with stochastic control flow. This paper also illustrates SPRJ MCMC using a tutorial example and an application to 3D Bayesian neuro-symbolic perception.",,,Marco Cusumano-Towner (Massachusetts Institute of Technology)*; Alexander K. Lew (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Cusumano-Towner, Marco*; Lew, Alexander K.; Mansinghka, Vikash",imarcoam@gmail.com*; alexlew@mit.edu; vkm@mit.edu,,Fri,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
57,1/10/2020 8:12:48 PM -08:00,1/10/2020 8:12:48 PM -08:00,TyXe : Pyro-Based Bayesian Neural Networks for Pytorch Users in 5 Lines of Code,"There has traditionally been strong interest in Bayesian Inference for Neural Networks in both the research and the data-science communities. However, ease of use has frequently held back widespread adoption of even the conceptually simpler models and algorithms due to the required coding overhead. Inspired by the user-friendliness of PyTorch and the expressivity of modern PPLs like Pyro, we propose EasyBNN as a way to easily ''bayesianize'' PyTorch neural networks with minimal coding efforts. In EasyBNN, users can both bayesianize pre-trained PyTorch neural networks and train them from scratch independent of architecture with a simple interface which offers most commonly used options for setup and inference of BNNs in as few as 5 lines of code including model definition. Our framework is consciously close to the common workflow of a typical deep learning practitioner, in particular when using PyTorch, and provides a minimalistic and useful interface to the world of Bayesian Inference.",,,Hippolyt Ritter (University College London); Theofanis Karaletsos (Uber AI Labs)*,"Ritter, Hippolyt; Karaletsos, Theofanis*",j.ritter@cs.ucl.ac.uk; theofanis.karaletsos@gmail.com*,,Fri,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis; The Practice of Probabilistic Programming,Extended Abstract,Accept,
58,1/10/2020 8:23:42 PM -08:00,1/10/2020 8:23:42 PM -08:00,A Language for Counterfactual Generative Models,"Probabilistic programming languages provide syntax to define and condition generative models but lack mechanisms for counterfactual queries.  We introduce OmegaC a causal probabilistic programming language for constructing and performing inference in counterfactual generative models.  In OmegaC, a counterfactual generative model is a program that combines both conditioning and causal interventions to represent queries such as """"given that X is true, what if Y were the case?"". We define the syntax and semantics of OmegaC and demonstrate examples in population dynamics, inverse planning and causation.",,,Zenna Tavares (MIT)*; James Koppel (MIT); Xin Zhang (MIT); Armando Solar-Lezama (MIT),"Tavares, Zenna*; Koppel, James; Zhang, Xin; Solar-Lezama, Armando",zenna@mit.edu*; jkoppel@mit.edu; xzhang@csail.mit.edu; asolar@csail.mit.edu,Yes,Thu,2,,Thu,"Languages, Tools, and Systems",,Extended Abstract,Talk,
59,1/10/2020 8:24:45 PM -08:00,1/10/2020 9:04:02 PM -08:00,A Probabilistic Programming Approach to Selection of TV Shows for Linear Advertising,"Advertising in television is done in linear format based on likely demographic characteristics of audiences of combinations of programs, networks, and airing times. This approach, however, doesn't take into account the viewership history of the TV audience.  In this abstract, we provide a data-driven approach for linear TV advertising that makes use of probabilistic programming through TensorFlow Probability toolbox in Python and builds a linear ad-placement recommender. This flexible modeling approach has the potential to add considerable value, both monetarily and in terms of business insight, to the linear advertising business in the longer term.",,,Ritwik Mitra (AT&T Labs - Research)*,"Mitra, Ritwik*",rmitra@research.att.com*,,Thu,2,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
60,1/10/2020 9:28:43 PM -08:00,1/10/2020 9:28:43 PM -08:00,A design proposal for InferenceQL: an SQL-like probabilistic programming language,"Many potential users of probabilistic inference do not need to know the details of the models that are being used to answer their questions, so long as they can build confidence that the results are reasonable. InferenceQL is an SQL-like probabilistic programming language that insulates users from knowledge of the underlying models and the data on which these models are based. InferenceQL includes a language for inference query plans, embedded in Clojure and Datalog, that also insulates users from the model representation. Both the SQL-like and Datalog-based languages in InferenceQL can specify workflows combining exploratory data analysis, inferential statistics, predictive modeling, and ad-hoc data transformations, all without referencing specific model parameters, model structure, or inference algorithms. The underlying models are represented as generative probabilistic programs that satisfy a simple black-box interface, and can sometimes be built automatically from data, using Bayesian probabilistic program synthesis techniques. InferenceQL is intended to be suitable for applications in which end users have the domain knowledge needed to pose meaningful questions, but lack the time or expertise needed to design and implement models and inference algorithms themselves. InferenceQL also may help teach concepts of conditional probability to a broad audience, and also make it easier to integrate sophisticated probabilistic programs into end-user applications and data pipelines. This abstract illustrates InferenceQL using the problem of detecting anomalies in a real-world database of Earth satellites.",,,Ulrich Schaechtle (MIT)*; Zane Shelby (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Schaechtle, Ulrich*; Shelby, Zane; Mansinghka, Vikash",ulli@mit.edu*; zshelby@mit.edu; vkm@mit.edu,,Fri,2,Ulrich Schaechtle,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
61,1/10/2020 9:44:10 PM -08:00,1/10/2020 9:44:10 PM -08:00,Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro,"NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX.In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes.",,,Du Phan (UIUC)*; Neeraj Pradhan (Uber AI Labs); Martin Jankowiak (Uber AI Labs),"Phan, Du*; Pradhan, Neeraj; Jankowiak, Martin",phandu@postech.ac.kr*; npradhan@uber.com; jankowiak@uber.com,,Fri,2,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
62,1/10/2020 10:06:30 PM -08:00,1/10/2020 10:06:30 PM -08:00,"Storm-IR: A Unified Framework for Analysis, Transformation, and Testing of Probabilistic Programs","Probabilistic programming languages intuitively represent models with uncertainty as simple programs. To allow software developers to write intuitive high-level code, the research and industry community developed numerous probabilistic languages. Despite a significant effort in developing core languages, efficient inference algorithms, and theory, much less attention has been devoted to practical systems for analyzing and transforming probabilistic programs and ensuring the correctness of the implementations of the probabilistic programming systems.  Researchers have also proposed various theoretical approaches for static analysis of probabilistic programs. However, these techniques are rarely implemented in commodity probabilistic (PPS). This is because various probabilistic programming systems differ some links widely in language models and inference strategies. To implement new analysis or transformation of probabilistic programs, a researcher needs to have a thorough understanding of the system to be able to implement any of these analyses/transformations – often an onerous and redundant task.  To address these challenges, we present the Storm-IR framework. Storm-IR allows defining new analyses and transformations on probabilistic programming languages. It exposes an intuitive intermediate representation that represents a Turing-complete, first-order probabilistic programming language. The programmer need only write the analyses and transformations on the level of our intermediate language using a simple API. To support operating on programs from popular languages we developed several front-ends and back-ends that allow us to operate on programs from languages such as Stan, Pyro, PSI, and Edward2.",,,Saikat Dutta (UIUC)*; Zixin Huang (UIUC); Jacob Laurel (University of Illinois Urbana Champaign); Sasa Misailovic (UIUC),"Dutta, Saikat*; Huang, Zixin; Laurel, Jacob; Misailovic, Sasa",saikatd2@illinois.edu*; zixinh2@illinois.edu; jlaurel2@illinois.edu; misailo@illinois.edu,,,,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Syndicated Submission,Withdrawn,
63,1/11/2020 1:17:35 AM -08:00,1/11/2020 2:36:26 AM -08:00,Attention for Inference Compilation,"We present a new neural network architecture for automatic amortized inference in universal probabilistic programs which improves on the performance of current architectures. Our approach extends inference compilation (IC), technique which uses deep neural networks to approximate a posterior distribution over latent variables in a probabilistic program. A challenge with existing IC network architectures is that they can fail to capture long-range dependencies between latent variables. To address this, we introduce an attention mechanism that attends to the most salient variables previously sampled in the execution of a probabilistic program. We demonstrate that the addition of attention allows the proposal distributions to better match the true posterior, enhancing inference about latent variables in programs.",,,William S G Harvey (University of British Columbia)*; Andreas Munk (University of British Columbia); Atilim Gunes Baydin (University of Oxford); Alexander Bergholm ( University of British Columbia); Frank Wood (University of British Columbia),"Harvey, William S G*; Munk, Andreas; Baydin, Atilim Gunes; Bergholm, Alexander; Wood, Frank",williamharvey08@gmail.com*; amunk@cs.ubc.ca; gunes@robots.ox.ac.uk; bergholm.alexander@gmail.com; fwood@cs.ubc.ca,,Fri,2,,,Statistics and Data Analysis,Artificial and Natural Intelligence,Extended Abstract,Accept,
64,1/11/2020 1:47:55 AM -08:00,1/11/2020 1:47:55 AM -08:00,Amortized Rejection Sampling in Universal Probabilistic Programming,Naive approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. This is particularly true on importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure.  In this paper we develop a new and efficient amortized importance sampling estimator.  We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.,,,Saeid Naderiparizi (University of British Columbia)*; Adam Scibior (University of British Columbia); Andreas Munk (University of British Columbia); Mehrdad Ghadiri (GeorgiaTech); Atilim Gunes Baydin (University of Oxford); Bradley J Gram-Hansen (University of Oxford); Christian A Schroeder de Witt (University of Oxford); Rob Zinkov (University of Oxford); Philip Torr (University of Oxford); Tom Rainforth (University of Oxford); Yee Whye Teh (University of Oxford); Frank Wood (University of British Columbia),"Naderiparizi, Saeid*; Scibior, Adam; Munk, Andreas; Ghadiri, Mehrdad; Baydin, Atilim Gunes; Gram-Hansen, Bradley J; Schroeder de Witt, Christian A; Zinkov, Rob; Torr, Philip; Rainforth, Tom; Teh, Yee Whye; Wood, Frank",saeidnp@cs.ubc.ca*; ascibior@cs.ubc.ca; amunk@cs.ubc.ca; ghadiri@gatech.edu; gunes@robots.ox.ac.uk; bradley@robots.ox.ac.uk; christian.schroeder@stcatz.ox.ac.uk; zinkov@robots.ox.ac.uk; philip.torr@eng.ox.ac.uk; rainforth@stats.ox.ac.uk; y.w.teh@stats.ox.ac.uk; fwood@cs.ubc.ca,,Fri,2,,,The Practice of Probabilistic Programming,Statistics and Data Analysis,Extended Abstract,Accept,
65,1/11/2020 1:55:08 AM -08:00,1/11/2020 2:10:33 AM -08:00,Assessing Re-Identification Risks using Probabilistic Programming (Extended Abstract),"Non-archival extended abstract, see pdf.",,,Raúl Pardo (IT University of Copenhagen)*; Willard Rafnsson (IT University of Copenhagen); Christian Probst (Unitec Institute of Technology); Andrzej Wąsowski (IT University of Copenhagen),"Pardo, Raúl*; Rafnsson, Willard; Probst, Christian; Wąsowski, Andrzej",raup@itu.dk*; wilr@itu.dk; cwprobst@gmail.com; wasowski@itu.dk,,Fri,2,,,Data Privacy,"Languages, Tools, and Systems; The Practice of Probabilistic Programming",Extended Abstract,Accept,
66,1/11/2020 2:09:11 AM -08:00,1/11/2020 2:09:11 AM -08:00,Auxiliary-Variable Programmable Inference,"Practitioners in diverse fields, including statistics, biology, and machine learning, rely on a wide variety of algorithms for exact and approximate probabilistic inference. Probabilistic programming systems with programmable inference [Bingham et al. 2019; Cusumano-Towner et al. 2019; Ge et al. 2018; Lew et al. 2019; Mansinghka et al. 2018, 2014; Murray 2013; Sennesh et al. 2018; Zinkov and Shan 2016] aim to simplify and accelerate the implementation of sophisticated models and inference algorithms: just as deep learning frameworks provide high-level abstractions for constructing neural networks and automate the computation of gradients in them, programmable inference systems provide support for constructing probabilistic models and inference algorithms, and automate the derivation of quantities important for probabilistic inference, such as MetropolisHastings acceptance probabilities and gradient estimates for variational inference. But most existing programmable inference systems come with a severe restriction: the random variables sampled in user-written variational families and proposal distributions must be in one-to-one correspondence with those in the model. Cusumano-Towner and Mansinghka [2018] discuss an importance-sampling technique to relax this restriction in Monte Carlo algorithms; the Gen [Cusumano-Towner et al. 2019] and ProbTorch [Sennesh et al. 2018] systems use a technique equivalent to the one-sample variant. This abstract presents a more flexible approach to lifting this auxiliary-variable restriction, which includes prior techniques as special cases. Our aim is to broaden the class of inference algorithms for which programmable inference systems can automate the math.",,,Alexander K. Lew (MIT)*; Benjamin Sherman (MIT); Marco Cusumano-Towner (); Michael Carbin (MIT); Vikash Mansinghka (Massachusetts Institute of Technology),"Lew, Alexander K.*; Sherman, Benjamin; Cusumano-Towner, Marco; Carbin, Michael; Mansinghka, Vikash",alexlew@mit.edu*; sherman@csail.mit.edu; marcoct@mit.edu; mcarbin@csail.mit.edu; vkm@mit.edu,,Fri,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
67,1/11/2020 3:13:50 AM -08:00,1/11/2020 3:13:50 AM -08:00,Almost surely terminating probabilistic programs are differentiable almost everywhere,"We address a theoretical question asked by H. Yang about the differentiability of densities encoded as probabilistic programs. Specifically, we show that for programs of a general language with recursion and higher-order functions, almost-sure termination implies that the density is differentiable except on a measure-zero set. This result is necessary in order to prove correctness properties of versions of HMC and variational inference, but we keep these applications for further work. ",,,Carol Mak (University of Oxford); Luke Ong (University of Oxford); Hugo Paquet (University of Oxford)*,"Mak, Carol; Ong, Luke; Paquet, Hugo*",carol.mak@magd.ox.ac.uk; luke.ong@cs.ox.ac.uk; hugo.paquet@cs.ox.ac.uk*,Yes,Fri,2,,Fri,"Languages, Tools, and Systems",,Extended Abstract,Talk,"Carol Mak poster, Hugo Paquet live presentation"
69,8/26/2020 2:36:00 PM -07:00,9/14/2020 11:28:39 PM -07:00,Probabilistic Programming with Lea,"Lea is a Python toolkit providing a PP framework for modeling discrete random variables and their possible dependencies. 
 Lea provides a small set of building blocks for defining discrete probability distributions, including categorical / joint probability distributions, conditional probability tables, Bayesian network and Markov chains. Queries involving arithmetical/logical/functional expressions can then be made, with possible conditioning, resulting in new probability distributions. Usual statistics indicators are provided, as well as the main measures of information theory.
 As a PPL, Lea promotes genericity and usability. In particular, beside usual floating-point representation for probabilities, it is open to other types like fractions, decimals and symbols, i.e. producing probability formula instead of numbers. This makes it specially useful for education, in the domains of probability theory and PP.
 The default inference engine uses a new exact algorithm, the Statues algorithm, which is based on the generator construct. For intractable problems, the user may opt for an approximate algorithm like Monte-Carlo rejection or weighted likelihood.
 In this talk, I propose to introduce the Lea PPL with a set of toy problems, ranging from traditional to less conventional.
 Lea project page: http://bitbucket.org/piedenis/lea/src/dev_lea3",Pierre Denis,pie.denis@skynet.be,Pierre Denis (independent scholar)*,"Denis, Pierre*",pie.denis@skynet.be*,,Fri,2,,,"Languages, Tools, and Systems",The Practice of Probabilistic Programming,Extended Abstract,Accept,
70,9/3/2020 10:17:27 AM -07:00,9/14/2020 2:40:56 AM -07:00,Probabilistic Programs with Stochastic Conditioning,"We propose to distinguish between deterministic conditioning, that is, conditioning on a sample from the joint data distribution, and stochastic conditioning, that is, conditioning on the distribution of the observable variable. Mostly, probabilistic programs follow the Bayesian approach by choosing a prior distribution of parameters and conditioning on observations. In a basic setting, individual observations are In a basic setting, individual observations are samples from the joint data distribution. However, observations may also be independent samples from marginal data distributions of each observable variable, summary statistics, or even data distributions themselves . These cases naturally appear in real life scenarios: samples from marginal distributions arise when different observations are collected by different parties, summary statistics (mean, variance, and quantiles) are often used to represent data collected over a large population, and data distributions may represent uncertainty during inference about future states of the world, that is, in planning. Probabilistic programming languages and frameworks which support conditioning on samples from the joint data distribution are not directly capable of expressing such models. We define the notion of stochastic conditioning and describe extensions of known general inference algorithms to probabilistic programs with stochastic conditioning. In case studies we provide probabilistic programs for several problems of statistical inference which are impossible or difficult to approach otherwise, perform inference on the programs, and analyse the results.",David Tolpin,david.tolpin@gmail.com,David Tolpin (Ben Gurion University of the Negev & PUB+)*; Hongseok Yang (KAIST); Yuan Zhou (University of Oxford),"Tolpin, David*; Yang, Hongseok; Zhou, Yuan",david.tolpin@gmail.com*; hongseok00@gmail.com; yuan.zhou@cs.ox.ac.uk,,Thu,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
71,9/11/2020 3:56:59 AM -07:00,9/11/2020 3:56:59 AM -07:00,EinStein VI: General Stein Variational Inference in NumPyro,"Stein Variational Inference is a technique for approximate Bayesian inferencethat is recently gaining popularity since it combines the scalability of traditionalVariational Inference with flexibility of non-parametric particle based inferencemethods. While there has been considerable progress in development of algorithms,integration in existing probabilistic programming languages (PPLs) with an easy-to-use interface is currently lacking. EinStein VI is a lightweight composable library that integrates the latest Stein Variational Inference methods with the NumPyro PPL. Inference with EinStein VI relies on ELBO-within-Stein to support use of custom inference programs (guides), supports non-linear scaling of repulsion force and supports second-order gradient updates using matrix-valued kernels.",Ahmad Salim Al-Sibahi,ahmad@di.ku.dk,Ahmad Salim Al-Sibahi (University of Copenhagen)*; Ola Rønning (University of Copenhagen); Christophe Ley (Ghent University); Thomas Hamelryck (University of Copenhagen),"Al-Sibahi, Ahmad Salim*; Rønning, Ola; Ley, Christophe; Hamelryck, Thomas",ahmad@di.ku.dk*; ola@di.ku.dk; Christophe.Ley@UGent.be; thamelry@binf.ku.dk,,Fri,2,,,The Practice of Probabilistic Programming,Artificial and Natural Intelligence,Extended Abstract,Accept,
72,9/12/2020 10:28:20 PM -07:00,9/14/2020 2:41:34 AM -07:00,Bayesian Policy Search for Stochastic Domains,"AI planning can be cast as inference in probabilistic models, and probabilistic programming was shown to be capable of policy search in partially observable domains. Prior work introduces policy search through Markov chain Monte Carlo in deterministic domains, as well as adapts black-box variational inference to stochastic domains, however not in the strictly Bayesian sense. In this work, we cast policy search in stochastic domains as a Bayesian inference problem and provide a scheme for encoding such problems as nested probabilistic programs. We argue that probabilistic programs for policy search in stochastic domains should involve nested conditioning, and provide an adaption of Lightweight Metropolis-Hastings (LMH) for robust inference in such programs. We apply the proposed scheme to stochastic domains and show that policies of similar quality are learned, despite a simpler and more general inference algorithm. We believe that the proposed variant of LMH is novel and applicable to a wider class of probabilistic programs with nested conditioning.",David Tolpin,david.tolpin@gmail.com,David Tolpin (Ben Gurion University of the Negev & PUB+)*; Yuan Zhou (University of Oxford); Hongseok Yang (KAIST),"Tolpin, David*; Zhou, Yuan; Yang, Hongseok",david.tolpin@gmail.com*; yuan.zhou@cs.ox.ac.uk; hongseok00@gmail.com,,Fri,1,,,Statistics and Data Analysis,Artificial and Natural Intelligence; The Practice of Probabilistic Programming,Extended Abstract,Accept,
73,9/12/2020 10:35:46 PM -07:00,9/12/2020 10:35:46 PM -07:00,Translating Recursive Probabilistic Programs to Factor Graph Grammars,"It is natural for probabilistic programs to use conditionals to express alternative substructures in models, and loops (recursion) to express repeated substructures in models. Thus, probabilistic programs with conditionals and recursion motivate ongoing interest in efficient and general inference. A _factor graph grammar_ (FGG) generates a set of factor graphs that do not all need to be enumerated in order to perform inference. We provide a semantics-preserving translation from first-order probabilistic programs with conditionals and recursion to FGGs.",Chung-chieh Shan,ccshan@indiana.edu,David Chiang (University of Notre Dame); Chung-chieh Shan (Indiana University)*,"Chiang, David; Shan, Chung-chieh*",dchiang@nd.edu; ccshan@indiana.edu*,Yes,Thu,2,,Sat,"Languages, Tools, and Systems",,Extended Abstract,Talk,
74,9/13/2020 9:28:14 AM -07:00,9/13/2020 4:08:53 PM -07:00,Dynamic specialization for trace-based probabilistic programming systems,"Modern probabilistic programming systems trade off flexibility in the expression of models for efficiency of inference. Trace-based systems such as Gen and Pyro support universal probabilistic programming - able to express any computable distribution by allowing the specification of probability measures over abstract
 dictionaries from addresses to random choices. In these systems, given the flexibility of expression, the primitive interfaces required to implement iterative inference algorithms incur a heavy cost - typically, the tracer must re-execute the entire program, including computations which are not required to compute an
 update or re-propose to a desired set of addresses. Despite this cost, these interfaces are crucial to the expression of effective inference algorithms in the style of programmable inference. To address this issue, modern systems like Gen provide specialized languages with multi-stage programming to generate efficient interpreters for update primitives. In this work, we present an alternative multi-stage technique which can also produce optimized interpreters for the primitive operations. In addition, we expect this technique to scale to dynamic programs with control flow.",McCoy R Becker,mccoybecker@gmail.com,McCoy R Becker (Charles River Analytics)*,"Becker, McCoy R*",mccoybecker@gmail.com*,,Thu,2,,,"Languages, Tools, and Systems",Artificial and Natural Intelligence,Extended Abstract,Accept,
76,9/14/2020 3:48:23 AM -07:00,9/15/2020 2:31:04 AM -07:00,SYMPAIS: Symbolic Adaptive Importance Sampling for Probabilistic Program Analysis,"Probabilistic software analysis aims at quantifying the probability of a target event occurring during the execution of a program processing uncertain incoming data or written itself using probabilistic programming constructs. Recent techniques combine classic static analysis methods with inference procedure to obtain accurate quantification of the probability of rare target events, such as failures in a mission-critical system. However, current techniques face several scalability and applicability limitations when analyzing software processing with high-dimensional multivariate distributions. In this paper, we present SYMbolic Parallel Adaptive Importance Sampling (SYMPAIS), a new algorithm that combines symbolic execution with adaptive importance sampling to analyze probabilistic programs. 
 Our method provides a general solution that scales to systems with high-dimensional inputs and demonstrates superior performance in
 quantifying rare events compared to prior work. 
 Preliminary experimental results support the potential efficacy of our solution.",Yicheng Luo,yicheng.luo.20@ucl.ac.uk,Yicheng Luo (Imperial College London)*; Antonio Filieri (Imperial College London); Yuan Zhou (University of Oxford),"Luo, Yicheng*; Filieri, Antonio; Zhou, Yuan",yicheng.luo.20@ucl.ac.uk*; a.filieri@imperial.ac.uk; yuan.zhou@cs.ox.ac.uk,,Fri,2,,,"Languages, Tools, and Systems",Statistics and Data Analysis,Extended Abstract,Accept,
77,9/14/2020 1:45:03 PM -07:00,9/14/2020 1:45:03 PM -07:00,Inverse Cognition in Search and Rescue using Probabilsitic Programming,"In this work, we demonstrate the effectiveness of probabilsitic programming as a means of recognizing the
 mental state of individuals as they interact with a complex environment. In particular, we create a simulation
 of search and rescue environment using the online game Minecraft and demonstrate our ability to infer the
 intent and predict the actions of an individual by observing their current movements and interactions with
 said environemnt.",Kenneth Lu,klu@cra.com,Kenneth Lu (Charles River Analytics)*,"Lu, Kenneth*",klu@cra.com*,,Fri,2,,,The Practice of Probabilistic Programming,Artificial and Natural Intelligence,Extended Abstract,Accept,
78,9/14/2020 2:17:11 PM -07:00,9/14/2020 2:39:33 PM -07:00,The Base Measure Problem and its Solution,"Probabilistic programming systems generally compute with probability density functions, leaving the base measure of each such function implicit. This usually works, but creates problems in situations where densities with respect to different base measures are accidentally combined or compared. We motivate and clarify the problem in the context of a composable library of probability distributions and bijective transformations. We also propose to solve the problem by standardizing on Hausdorff measure as a base, and by deriving a formula and software architecture for updating densities with respect to Hausdorff measure under diffeomorphic transformations. We hope that by adopting our solution, probabilistic programming systems can become more robust and general, and make a broader class of models accessible to practitioners.",Alexey Radul,axch@google.com,Alexey Radul (Google)*; Boris Alexeev (Google),"Radul, Alexey*; Alexeev, Boris",axch@google.com*; alexeev@google.com,,Thu,2,,,"Languages, Tools, and Systems",,Extended Abstract,Accept,
79,9/14/2020 6:12:42 PM -07:00,9/14/2020 6:12:42 PM -07:00,Towards Causal Psychophysiology in the Wild: Probabilistic Programs for Skin Conductance Analysis,"Electrodermal Activity (EDA) is a common physiological marker for identifying changes in mental states. While this underlying relationship is easily identified under controlled lab conditions, insights from real-world use remain difficult due to many noise sources, artifacts, and confounding variables. In this paper, we bring probabilistic programming and stochastic variational inference to bear on realistic recordings of EDA data. Combined with a probabilistic treatment of other physiological indicators, we believe this approach will improve the state-of-the-art in measurement and prediction of everyday phenomenology.",David Ramsay,dramsay@mit.edu,David Ramsay (MIT Media Lab)*; Patrick Chwalek (MIT Media Lab); Jan-Willem van de Meent (Northeastern University); Joseph Paradiso (MIT Media Lab),"Ramsay, David*; Chwalek, Patrick; van de Meent, Jan-Willem; Paradiso, Joseph",dramsay@mit.edu*; chwalek@mit.edu; j.vandemeent@northeastern.edu; joep@media.mit.edu,,Thu,2,,,The Practice of Probabilistic Programming,,Extended Abstract,Accept,
80,9/14/2020 7:29:05 PM -07:00,9/14/2020 7:29:05 PM -07:00,Bayesian classification and modeling of single-molecule fluorescence images using Pyro,"Multi-wavelength single-molecule fluorescence microscopy methods allow elucidation of complex biochemical reaction mechanisms. However, single molecule image data are difficult to analyze due to low signal-to-noise ratio and non-specific binding in such experiments. To overcome these difficulties, we developed a new analysis method which is based on probabilistic modeling, statistically rigorous, and scalable to large datasets. The model and inference algorithm was implemented in Pyro probabilistic programming language.",Yerdos Ordabayev,ordabayev@brandeis.edu,Yerdos Ordabayev (Brandeis University)*; Larry Friedman (Brandeis University); Douglas Theobald (Brandeis University); Jeff Gelles (Brandeis University),"Ordabayev, Yerdos*; Friedman, Larry; Theobald, Douglas; Gelles, Jeff",ordabayev@brandeis.edu*; larryfj@brandeis.edu; dtheobald@brandeis.edu; gelles@brandeis.edu,,Thu,2,Yerdos Ordabayev,,Statistics and Data Analysis,The Practice of Probabilistic Programming,Extended Abstract,Accept,
82,9/14/2020 9:22:53 PM -07:00,9/14/2020 9:22:53 PM -07:00,Structural time series grammar over variable blocks,"A structural time series model additively decomposes into generative, semantically-meaningful 
 components, each of which depends on a vector of parameters.
 We demonstrate that considering each generative component together with its 
 vector of parameters as a single latent structural time series node can simplify reasoning about collections of structural time series components. 
 We then introduce a formal grammar over structural time series nodes and parameter vectors.
 Valid sentences in the grammar can be interpreted as generative structural time series models.
 An extension of the grammar can also express structural time series models that include changepoints, though these models are necessarily not generative. 
 We demonstrate a preliminary implementation of the language generated by this 
 grammar.
 We close with a discussion of possible future work.",DavId R Dewhurst,ddewhurst@cra.com,DavId R Dewhurst (Charles River Analytics)*,"Dewhurst, DavId R*",ddewhurst@cra.com*,,Fri,2,,,Statistics and Data Analysis,"Languages, Tools, and Systems",Extended Abstract,Accept,
83,9/15/2020 3:54:02 AM -07:00,9/15/2020 3:54:02 AM -07:00,An embeddable uncertainty module for strategy game,"Current strategy games lack a decent system to denote uncertainty, this work presents a mechanism using Stheno.jl, a Julia package for GPPP (Gaussian process Probabilistic Programming), to develop a system to model uncertainty. Some correspondences between GP primitives and game entities are discussed. A preliminary case study about the historical Battle of Coral Sea is also included to show the usage.",Yueyi Zhuo,yiyuezhuo@gmail.com,Yueyi Zhuo (NJUST)*,"Zhuo, Yueyi*",yiyuezhuo@gmail.com*,,Fri,2,,,The Practice of Probabilistic Programming,,Extended Abstract,Accept,